{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¯å‘é‡è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œè¯å‘é‡æ˜¯è¡¨ç¤ºè‡ªç„¶è¯­è¨€é‡Œå•è¯çš„ä¸€ç§æ–¹æ³•ï¼Œå³æŠŠæ¯ä¸ªè¯éƒ½è¡¨ç¤ºä¸ºä¸€ä¸ªNç»´ç©ºé—´å†…çš„ç‚¹ï¼Œå³ä¸€ä¸ªé«˜ç»´ç©ºé—´å†…çš„å‘é‡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå®ç°æŠŠè‡ªç„¶è¯­è¨€è®¡ç®—è½¬æ¢ä¸ºå‘é‡è®¡ç®—ã€‚\n",
    "\n",
    "å¦‚ **å›¾1** æ‰€ç¤ºçš„è¯å‘é‡è®¡ç®—ä»»åŠ¡ä¸­ï¼Œå…ˆæŠŠæ¯ä¸ªè¯ï¼ˆå¦‚queenï¼Œkingç­‰ï¼‰è½¬æ¢æˆä¸€ä¸ªé«˜ç»´ç©ºé—´çš„å‘é‡ï¼Œè¿™äº›å‘é‡åœ¨ä¸€å®šæ„ä¹‰ä¸Šå¯ä»¥ä»£è¡¨è¿™ä¸ªè¯çš„è¯­ä¹‰ä¿¡æ¯ã€‚å†é€šè¿‡è®¡ç®—è¿™äº›å‘é‡ä¹‹é—´çš„è·ç¦»ï¼Œå°±å¯ä»¥è®¡ç®—å‡ºè¯è¯­ä¹‹é—´çš„å…³è”å…³ç³»ï¼Œä»è€Œè¾¾åˆ°è®©è®¡ç®—æœºåƒè®¡ç®—æ•°å€¼ä¸€æ ·å»è®¡ç®—è‡ªç„¶è¯­è¨€çš„ç›®çš„ã€‚\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/401409b5b36f4c55ade6078ba198634ee54b8f27e51d4729b66b32e844689969\" width=\"1000\" ></center>\n",
    "<center>å›¾1ï¼šè¯å‘é‡è®¡ç®—ç¤ºæ„å›¾</center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vecåŒ…å«ä¸¤ä¸ªç»å…¸æ¨¡å‹ï¼ŒCBOWï¼ˆContinuous Bag-of-Wordsï¼‰å’ŒSkip-gramï¼Œå¦‚ **å›¾2** æ‰€ç¤ºã€‚\n",
    "\n",
    "- **CBOW**ï¼šé€šè¿‡ä¸Šä¸‹æ–‡çš„è¯å‘é‡æ¨ç†ä¸­å¿ƒè¯ã€‚\n",
    "- **Skip-gram**ï¼šæ ¹æ®ä¸­å¿ƒè¯æ¨ç†ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5d0fc1ad327e438e9101d059772be6732ee808bd80e04e1eabc575cd28e61a69\" width=\"1000\" ></center>\n",
    "<center><br>å›¾2ï¼šCBOWå’ŒSkip-gramè¯­ä¹‰å­¦ä¹ ç¤ºæ„å›¾</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOWçš„ç®—æ³•å®ç°\n",
    "\n",
    "æˆ‘ä»¬ä»¥è¿™å¥è¯ï¼šâ€œPineapples are spiked and yellowâ€ä¸ºä¾‹ä»‹ç»CBOWç®—æ³•å®ç°ã€‚\n",
    "\n",
    "å¦‚ **å›¾3** æ‰€ç¤ºï¼ŒCBOWæ˜¯ä¸€ä¸ªå…·æœ‰3å±‚ç»“æ„çš„ç¥ç»ç½‘ç»œï¼Œåˆ†åˆ«æ˜¯ï¼š\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/72397490c0ba499692cff31484431c57bc9d20f7ef344454868e12d628ec5bd3\" width=\"400\" ></center>\n",
    "<center><br>å›¾3ï¼šCBOWçš„ç®—æ³•å®ç°</br></center>\n",
    "<br></br>\n",
    "\n",
    "* **è¾“å…¥å±‚ï¼š** ä¸€ä¸ªå½¢çŠ¶ä¸ºCÃ—Vçš„one-hotå¼ é‡ï¼Œå…¶ä¸­Cä»£è¡¨ä¸Šçº¿æ–‡ä¸­è¯çš„ä¸ªæ•°ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå¶æ•°ï¼Œæˆ‘ä»¬å‡è®¾ä¸º4ï¼›Vè¡¨ç¤ºè¯è¡¨å¤§å°ï¼Œæˆ‘ä»¬å‡è®¾ä¸º5000ï¼Œè¯¥å¼ é‡çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡è¯çš„one-hotå‘é‡è¡¨ç¤ºï¼Œæ¯”å¦‚â€œPineapples, are, and, yellowâ€ã€‚\n",
    "* **éšè—å±‚ï¼š** ä¸€ä¸ªå½¢çŠ¶ä¸ºVÃ—Nçš„å‚æ•°å¼ é‡W1ï¼Œä¸€èˆ¬ç§°ä¸ºword-embeddingï¼ŒNè¡¨ç¤ºæ¯ä¸ªè¯çš„è¯å‘é‡é•¿åº¦ï¼Œæˆ‘ä»¬å‡è®¾ä¸º128ã€‚è¾“å…¥å¼ é‡å’Œword embedding W1è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå°±ä¼šå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸ºCÃ—Nçš„å¼ é‡ã€‚ç»¼åˆè€ƒè™‘ä¸Šä¸‹æ–‡ä¸­æ‰€æœ‰è¯çš„ä¿¡æ¯å»æ¨ç†ä¸­å¿ƒè¯ï¼Œå› æ­¤å°†ä¸Šä¸‹æ–‡ä¸­Cä¸ªè¯ç›¸åŠ å¾—ä¸€ä¸ª1Ã—Nçš„å‘é‡ï¼Œæ˜¯æ•´ä¸ªä¸Šä¸‹æ–‡çš„ä¸€ä¸ªéšå«è¡¨ç¤ºã€‚\n",
    "* **è¾“å‡ºå±‚ï¼š** åˆ›å»ºå¦ä¸€ä¸ªå½¢çŠ¶ä¸ºNÃ—Vçš„å‚æ•°å¼ é‡ï¼Œå°†éšè—å±‚å¾—åˆ°çš„1Ã—Nçš„å‘é‡ä¹˜ä»¥è¯¥NÃ—Vçš„å‚æ•°å¼ é‡ï¼Œå¾—åˆ°äº†ä¸€ä¸ªå½¢çŠ¶ä¸º1Ã—Vçš„å‘é‡ã€‚æœ€ç»ˆï¼Œ1Ã—Vçš„å‘é‡ä»£è¡¨äº†ä½¿ç”¨ä¸Šä¸‹æ–‡å»æ¨ç†ä¸­å¿ƒè¯ï¼Œæ¯ä¸ªå€™é€‰è¯çš„æ‰“åˆ†ï¼Œå†ç»è¿‡softmaxå‡½æ•°çš„å½’ä¸€åŒ–ï¼Œå³å¾—åˆ°äº†å¯¹ä¸­å¿ƒè¯çš„æ¨ç†æ¦‚ç‡ï¼š\n",
    "\n",
    "$$ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥({O_i})= \\frac{exp({O_i})}{\\sum_jexp({O_j})}$$\n",
    "\n",
    "åœ¨å®é™…æ“ä½œä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼ˆä¸€èˆ¬æƒ…å†µä¸‹ï¼Œé•¿åº¦æ˜¯å¥‡æ•°ï¼‰ï¼Œä»å·¦åˆ°å³å¼€å§‹æ‰«æå½“å‰å¥å­ã€‚æ¯ä¸ªæ‰«æå‡ºæ¥çš„ç‰‡æ®µè¢«å½“æˆä¸€ä¸ªå°å¥å­ï¼Œæ¯ä¸ªå°å¥å­ä¸­é—´çš„è¯è¢«è®¤ä¸ºæ˜¯ä¸­å¿ƒè¯ï¼Œå…¶ä½™çš„è¯è¢«è®¤ä¸ºæ˜¯è¿™ä¸ªä¸­å¿ƒè¯çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "### CBOWçš„å®é™…å®ç°\n",
    "\n",
    "åœ¨å®é™…ä¸­ï¼Œä¸ºé¿å…è¿‡äºåºå¤§çš„è®¡ç®—é‡ï¼Œæˆ‘ä»¬é€šå¸¸é‡‡ç”¨è´Ÿé‡‡æ ·çš„æ–¹æ³•ï¼Œæ¥é¿å…æŸ¥è¯¢æ•´ä¸ªæ­¤è¡¨ï¼Œä»è€Œå°†å¤šåˆ†ç±»é—®é¢˜è½¬æ¢ä¸ºäºŒåˆ†ç±»é—®é¢˜ã€‚å…·ä½“å®ç°è¿‡ç¨‹**å¦‚å›¾6**ï¼š\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/517260aee7f24b97bb8e42774daf1b4ee5aaa5824d004ed1854a04bde4a1c8ac)\n",
    "<center><br>å›¾6ï¼šCBOWç®—æ³•çš„å®é™…å®ç°</br></center>\n",
    "<br></br>\n",
    "\n",
    "åœ¨å®ç°çš„è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä¼šè®©æ¨¡å‹æ¥æ”¶3ä¸ªtensorè¾“å…¥ï¼š\n",
    "\n",
    "- ä»£è¡¨ä¸Šä¸‹æ–‡å•è¯çš„tensorï¼šå‡è®¾æˆ‘ä»¬ç§°ä¹‹ä¸ºcontext_words $V$ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªtensoræ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[batch_size, vocab_size]çš„one-hot tensorï¼Œè¡¨ç¤ºåœ¨ä¸€ä¸ªmini-batchä¸­æ¯ä¸ªä¸­å¿ƒè¯å…·ä½“çš„IDã€‚\n",
    "\n",
    "- ä»£è¡¨ç›®æ ‡è¯çš„tensorï¼šå‡è®¾æˆ‘ä»¬ç§°ä¹‹ä¸ºtarget_words $T$ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªtensoråŒæ ·æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[batch_size, vocab_size]çš„one-hot tensorï¼Œè¡¨ç¤ºåœ¨ä¸€ä¸ªmini-batchä¸­æ¯ä¸ªç›®æ ‡è¯å…·ä½“çš„IDã€‚\n",
    "\n",
    "- ä»£è¡¨ç›®æ ‡è¯æ ‡ç­¾çš„tensorï¼šå‡è®¾æˆ‘ä»¬ç§°ä¹‹ä¸ºlabels $L$ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªtensoræ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[batch_size, 1]çš„tensorï¼Œæ¯ä¸ªå…ƒç´ ä¸æ˜¯0å°±æ˜¯1ï¼ˆ0ï¼šè´Ÿæ ·æœ¬ï¼Œ1ï¼šæ­£æ ·æœ¬ï¼‰ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨é£æ¡¨å®ç° CBOW\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ ä½¿ç”¨é£æ¡¨å®ç°CBOWæ¨¡å‹çš„æ–¹æ³•ã€‚åœ¨é£æ¡¨ä¸­ï¼Œä¸åŒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŸºæœ¬ä¸€è‡´ï¼Œæµç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. **æ•°æ®å¤„ç†**ï¼šé€‰æ‹©éœ€è¦ä½¿ç”¨çš„æ•°æ®ï¼Œå¹¶åšå¥½å¿…è¦çš„é¢„å¤„ç†å·¥ä½œã€‚\n",
    "\n",
    "2. **ç½‘ç»œå®šä¹‰**ï¼šä½¿ç”¨é£æ¡¨å®šä¹‰å¥½ç½‘ç»œç»“æ„ï¼ŒåŒ…æ‹¬è¾“å…¥å±‚ï¼Œä¸­é—´å±‚ï¼Œè¾“å‡ºå±‚ï¼ŒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚\n",
    "\n",
    "3. **ç½‘ç»œè®­ç»ƒ**ï¼šå°†å‡†å¤‡å¥½çš„æ•°æ®é€å…¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œå¹¶è§‚å¯Ÿå­¦ä¹ çš„è¿‡ç¨‹æ˜¯å¦æ­£å¸¸ï¼Œå¦‚æŸå¤±å‡½æ•°å€¼æ˜¯å¦åœ¨é™ä½ï¼Œä¹Ÿå¯ä»¥æ‰“å°ä¸€äº›ä¸­é—´æ­¥éª¤çš„ç»“æœå‡ºæ¥ç­‰ã€‚\n",
    "\n",
    "4. **ç½‘ç»œè¯„ä¼°**ï¼šä½¿ç”¨æµ‹è¯•é›†åˆæµ‹è¯•è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œï¼Œçœ‹çœ‹è®­ç»ƒæ•ˆæœå¦‚ä½•ã€‚\n",
    "\n",
    "åœ¨æ•°æ®å¤„ç†å‰ï¼Œéœ€è¦å…ˆåŠ è½½é£æ¡¨å¹³å°ï¼ˆå¦‚æœç”¨æˆ·åœ¨æœ¬åœ°ä½¿ç”¨ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…é£æ¡¨ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:38:45.087221Z",
     "start_time": "2022-07-13T09:38:42.970576Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:22.023816Z",
     "iopub.status.busy": "2022-07-13T08:01:22.023142Z",
     "iopub.status.idle": "2022-07-13T08:01:23.271266Z",
     "shell.execute_reply": "2022-07-13T08:01:23.270261Z",
     "shell.execute_reply.started": "2022-07-13T08:01:22.023782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®å¤„ç†\n",
    "\n",
    "é¦–å…ˆï¼Œæ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„è¯­æ–™ç”¨äºè®­ç»ƒword2vecæ¨¡å‹ã€‚æˆ‘ä»¬é€‰æ‹©text8æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†é‡ŒåŒ…å«äº†å¤§é‡ä»ç»´åŸºç™¾ç§‘æ”¶é›†åˆ°çš„è‹±æ–‡è¯­æ–™ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç ä¸‹è½½æ•°æ®é›†ï¼Œä¸‹è½½åçš„æ–‡ä»¶è¢«ä¿å­˜åœ¨å½“å‰ç›®å½•çš„text8.txtæ–‡ä»¶å†…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:08.428933Z",
     "start_time": "2022-07-13T09:38:45.088222Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:25.708232Z",
     "iopub.status.busy": "2022-07-13T08:01:25.707231Z",
     "iopub.status.idle": "2022-07-13T08:01:30.548619Z",
     "shell.execute_reply": "2022-07-13T08:01:30.547755Z",
     "shell.execute_reply.started": "2022-07-13T08:01:25.708195Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ä¸‹è½½è¯­æ–™ç”¨æ¥è®­ç»ƒword2vec\n",
    "def download():\n",
    "    #å¯ä»¥ä»ç™¾åº¦äº‘æœåŠ¡å™¨ä¸‹è½½ä¸€äº›å¼€æºæ•°æ®é›†ï¼ˆdataset.bj.bcebos.comï¼‰\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\n",
    "    #ä½¿ç”¨pythonçš„requestsåŒ…ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "    #æŠŠä¸‹è½½åçš„æ–‡ä»¶å­˜å‚¨åœ¨å½“å‰ç›®å½•çš„text8.txtæ–‡ä»¶å†…\n",
    "    with open(\"./text8.txt\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼ŒæŠŠä¸‹è½½çš„è¯­æ–™è¯»å–åˆ°ç¨‹åºé‡Œï¼Œå¹¶æ‰“å°å‰500ä¸ªå­—ç¬¦çœ‹çœ‹è¯­æ–™çš„æ ·å­ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:08.938083Z",
     "start_time": "2022-07-13T09:39:08.429441Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:35.085243Z",
     "iopub.status.busy": "2022-07-13T08:01:35.084263Z",
     "iopub.status.idle": "2022-07-13T08:01:35.269646Z",
     "shell.execute_reply": "2022-07-13T08:01:35.268851Z",
     "shell.execute_reply.started": "2022-07-13T08:01:35.085209Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "#è¯»å–text8æ•°æ®\n",
    "def load_text8():\n",
    "    with open(\"./text8.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = load_text8()\n",
    "\n",
    "#æ‰“å°å‰500ä¸ªå­—ç¬¦ï¼Œç®€è¦çœ‹ä¸€ä¸‹è¿™ä¸ªè¯­æ–™çš„æ ·å­\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œéœ€è¦å…ˆå¯¹è¯­æ–™è¿›è¡Œåˆ‡è¯ã€‚å¯¹äºè‹±æ–‡æ¥è¯´ï¼Œå¯ä»¥æ¯”è¾ƒç®€å•åœ°ç›´æ¥ä½¿ç”¨ç©ºæ ¼è¿›è¡Œåˆ‡è¯ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:09.700647Z",
     "start_time": "2022-07-13T09:39:08.938590Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:38.777836Z",
     "iopub.status.busy": "2022-07-13T08:01:38.777232Z",
     "iopub.status.idle": "2022-07-13T08:01:40.199516Z",
     "shell.execute_reply": "2022-07-13T08:01:40.198706Z",
     "shell.execute_reply.started": "2022-07-13T08:01:38.777789Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "#å¯¹è¯­æ–™è¿›è¡Œé¢„å¤„ç†ï¼ˆåˆ†è¯ï¼‰\n",
    "def data_preprocess(corpus):\n",
    "    #ç”±äºè‹±æ–‡å•è¯å‡ºç°åœ¨å¥é¦–çš„æ—¶å€™ç»å¸¸è¦å¤§å†™ï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠæ‰€æœ‰è‹±æ–‡å­—ç¬¦éƒ½è½¬æ¢ä¸ºå°å†™ï¼Œ\n",
    "    #ä»¥ä¾¿å¯¹è¯­æ–™è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆApple vs appleç­‰ï¼‰\n",
    "    corpus = corpus.strip().lower()\n",
    "    corpus = corpus.split(\" \")\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = data_preprocess(corpus)\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ç»è¿‡åˆ‡è¯åï¼Œéœ€è¦å¯¹è¯­æ–™è¿›è¡Œç»Ÿè®¡ï¼Œä¸ºæ¯ä¸ªè¯æ„é€ IDã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¯ä»¥æ ¹æ®æ¯ä¸ªè¯åœ¨è¯­æ–™ä¸­å‡ºç°çš„é¢‘æ¬¡æ„é€ IDï¼Œé¢‘æ¬¡è¶Šé«˜ï¼ŒIDè¶Šå°ï¼Œä¾¿äºå¯¹è¯å…¸è¿›è¡Œç®¡ç†ã€‚ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:12.903938Z",
     "start_time": "2022-07-13T09:39:09.701155Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:45.348350Z",
     "iopub.status.busy": "2022-07-13T08:01:45.347672Z",
     "iopub.status.idle": "2022-07-13T08:01:50.341158Z",
     "shell.execute_reply": "2022-07-13T08:01:50.340194Z",
     "shell.execute_reply.started": "2022-07-13T08:01:45.348312Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 253854 different words in the corpus\n",
      "word the, its id 0, its word freq 1061396\n",
      "word of, its id 1, its word freq 593677\n",
      "word and, its id 2, its word freq 416629\n",
      "word one, its id 3, its word freq 411764\n",
      "word in, its id 4, its word freq 372201\n",
      "word a, its id 5, its word freq 325873\n",
      "word to, its id 6, its word freq 316376\n",
      "word zero, its id 7, its word freq 264975\n",
      "word nine, its id 8, its word freq 250430\n",
      "word two, its id 9, its word freq 192644\n",
      "word is, its id 10, its word freq 183153\n",
      "word as, its id 11, its word freq 131815\n",
      "word eight, its id 12, its word freq 125285\n",
      "word for, its id 13, its word freq 118445\n",
      "word s, its id 14, its word freq 116710\n",
      "word five, its id 15, its word freq 115789\n",
      "word three, its id 16, its word freq 114775\n",
      "word was, its id 17, its word freq 112807\n",
      "word by, its id 18, its word freq 111831\n",
      "word that, its id 19, its word freq 109510\n",
      "word four, its id 20, its word freq 108182\n",
      "word six, its id 21, its word freq 102145\n",
      "word seven, its id 22, its word freq 99683\n",
      "word with, its id 23, its word freq 95603\n",
      "word on, its id 24, its word freq 91250\n",
      "word are, its id 25, its word freq 76527\n",
      "word it, its id 26, its word freq 73334\n",
      "word from, its id 27, its word freq 72871\n",
      "word or, its id 28, its word freq 68945\n",
      "word his, its id 29, its word freq 62603\n",
      "word an, its id 30, its word freq 61925\n",
      "word be, its id 31, its word freq 61281\n",
      "word this, its id 32, its word freq 58832\n",
      "word which, its id 33, its word freq 54788\n",
      "word at, its id 34, its word freq 54576\n",
      "word he, its id 35, its word freq 53573\n",
      "word also, its id 36, its word freq 44358\n",
      "word not, its id 37, its word freq 44033\n",
      "word have, its id 38, its word freq 39712\n",
      "word were, its id 39, its word freq 39086\n",
      "word has, its id 40, its word freq 37866\n",
      "word but, its id 41, its word freq 35358\n",
      "word other, its id 42, its word freq 32433\n",
      "word their, its id 43, its word freq 31523\n",
      "word its, its id 44, its word freq 29567\n",
      "word first, its id 45, its word freq 28810\n",
      "word they, its id 46, its word freq 28553\n",
      "word some, its id 47, its word freq 28161\n",
      "word had, its id 48, its word freq 28100\n",
      "word all, its id 49, its word freq 26229\n"
     ]
    }
   ],
   "source": [
    "#æ„é€ è¯å…¸ï¼Œç»Ÿè®¡æ¯ä¸ªè¯çš„é¢‘ç‡ï¼Œå¹¶æ ¹æ®é¢‘ç‡å°†æ¯ä¸ªè¯è½¬æ¢ä¸ºä¸€ä¸ªæ•´æ•°id\n",
    "def build_dict(corpus):\n",
    "    #é¦–å…ˆç»Ÿè®¡æ¯ä¸ªä¸åŒè¯çš„é¢‘ç‡ï¼ˆå‡ºç°çš„æ¬¡æ•°ï¼‰ï¼Œä½¿ç”¨ä¸€ä¸ªè¯å…¸è®°å½•\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    #å°†è¿™ä¸ªè¯å…¸ä¸­çš„è¯ï¼ŒæŒ‰ç…§å‡ºç°æ¬¡æ•°æ’åºï¼Œå‡ºç°æ¬¡æ•°è¶Šé«˜ï¼Œæ’åºè¶Šé å‰\n",
    "    #ä¸€èˆ¬æ¥è¯´ï¼Œå‡ºç°é¢‘ç‡é«˜çš„é«˜é¢‘è¯å¾€å¾€æ˜¯ï¼šIï¼Œtheï¼Œyouè¿™ç§ä»£è¯ï¼Œè€Œå‡ºç°é¢‘ç‡ä½çš„è¯ï¼Œå¾€å¾€æ˜¯ä¸€äº›åè¯ï¼Œå¦‚ï¼šnlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    #æ„é€ 3ä¸ªä¸åŒçš„è¯å…¸ï¼Œåˆ†åˆ«å­˜å‚¨ï¼Œ\n",
    "    #æ¯ä¸ªè¯åˆ°idçš„æ˜ å°„å…³ç³»ï¼šword2id_dict\n",
    "    #æ¯ä¸ªidå‡ºç°çš„é¢‘ç‡ï¼šword2id_freq\n",
    "    #æ¯ä¸ªidåˆ°è¯å…¸æ˜ å°„å…³ç³»ï¼šid2word_dict\n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "    id2word_dict = dict()\n",
    "\n",
    "    #æŒ‰ç…§é¢‘ç‡ï¼Œä»é«˜åˆ°ä½ï¼Œå¼€å§‹éå†æ¯ä¸ªå•è¯ï¼Œå¹¶ä¸ºè¿™ä¸ªå•è¯æ„é€ ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„id\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict)\n",
    "        word2id_dict[word] = curr_id\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "        id2word_dict[curr_id] = word\n",
    "\n",
    "    return word2id_freq, word2id_dict, id2word_dict\n",
    "\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¾—åˆ°word2idè¯å…¸åï¼Œæˆ‘ä»¬è¿˜éœ€è¦è¿›ä¸€æ­¥å¤„ç†åŸå§‹è¯­æ–™ï¼ŒæŠŠæ¯ä¸ªè¯æ›¿æ¢æˆå¯¹åº”çš„IDï¼Œä¾¿äºç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:14.328516Z",
     "start_time": "2022-07-13T09:39:12.905462Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:55.467687Z",
     "iopub.status.busy": "2022-07-13T08:01:55.467100Z",
     "iopub.status.idle": "2022-07-13T08:01:57.975794Z",
     "shell.execute_reply": "2022-07-13T08:01:57.975005Z",
     "shell.execute_reply.started": "2022-07-13T08:01:55.467649Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 tokens in the corpus\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580, 0, 194, 10, 190, 58, 4, 5, 10712, 214, 6, 1324, 104, 454, 19, 58, 2731, 362, 6, 3672, 0]\n"
     ]
    }
   ],
   "source": [
    "#æŠŠè¯­æ–™è½¬æ¢ä¸ºidåºåˆ—\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    #ä½¿ç”¨ä¸€ä¸ªå¾ªç¯ï¼Œå°†è¯­æ–™ä¸­çš„æ¯ä¸ªè¯æ›¿æ¢æˆå¯¹åº”çš„idï¼Œä»¥ä¾¿äºç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œéœ€è¦ä½¿ç”¨äºŒæ¬¡é‡‡æ ·æ³•å¤„ç†åŸå§‹æ–‡æœ¬ã€‚äºŒæ¬¡é‡‡æ ·æ³•çš„ä¸»è¦æ€æƒ³æ˜¯é™ä½é«˜é¢‘è¯åœ¨è¯­æ–™ä¸­å‡ºç°çš„é¢‘æ¬¡ï¼Œé™ä½çš„æ–¹æ³•æ˜¯éšæœºå°†é«˜é¢‘çš„è¯æŠ›å¼ƒï¼Œé¢‘ç‡è¶Šé«˜ï¼Œè¢«æŠ›å¼ƒçš„æ¦‚ç‡å°±è¶Šé«˜ï¼Œé¢‘ç‡è¶Šä½ï¼Œè¢«æŠ›å¼ƒçš„æ¦‚ç‡å°±è¶Šä½ï¼Œè¿™æ ·åƒæ ‡ç‚¹ç¬¦å·æˆ–å† è¯è¿™æ ·çš„é«˜é¢‘è¯å°±ä¼šè¢«æŠ›å¼ƒï¼Œä»è€Œä¼˜åŒ–æ•´ä¸ªè¯è¡¨çš„è¯å‘é‡è®­ç»ƒæ•ˆæœï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:25.358903Z",
     "start_time": "2022-07-13T09:39:14.329518Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:02:03.856561Z",
     "iopub.status.busy": "2022-07-13T08:02:03.855655Z",
     "iopub.status.idle": "2022-07-13T08:02:14.220977Z",
     "shell.execute_reply": "2022-07-13T08:02:14.220151Z",
     "shell.execute_reply.started": "2022-07-13T08:02:03.856528Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8743238 tokens in the corpus\n",
      "[5233, 3080, 11, 194, 3133, 155, 741, 10571, 27349, 854, 15067, 58112, 854, 3580, 194, 10712, 214, 1324, 454, 58, 2731, 362, 3672, 708, 539, 1423, 2757, 567, 686, 7088, 5233, 1052, 0, 320, 248, 44611, 2877, 792, 5233, 602, 1134, 2621, 8983, 279, 4147, 141, 25, 6437, 4186, 5233]\n"
     ]
    }
   ],
   "source": [
    "#ä½¿ç”¨äºŒæ¬¡é‡‡æ ·ç®—æ³•ï¼ˆsubsamplingï¼‰å¤„ç†è¯­æ–™ï¼Œå¼ºåŒ–è®­ç»ƒæ•ˆæœ\n",
    "def subsampling(corpus, word2id_freq):\n",
    "    \n",
    "    #è¿™ä¸ªdiscardå‡½æ•°å†³å®šäº†ä¸€ä¸ªè¯ä¼šä¸ä¼šè¢«æ›¿æ¢ï¼Œè¿™ä¸ªå‡½æ•°æ˜¯å…·æœ‰éšæœºæ€§çš„ï¼Œæ¯æ¬¡è°ƒç”¨ç»“æœä¸åŒ\n",
    "    #å¦‚æœä¸€ä¸ªè¯çš„é¢‘ç‡å¾ˆå¤§ï¼Œé‚£ä¹ˆå®ƒè¢«é—å¼ƒçš„æ¦‚ç‡å°±å¾ˆå¤§\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\n",
    "\n",
    "    corpus = [word for word in corpus if not discard(word)]\n",
    "    return corpus\n",
    "\n",
    "corpus = subsampling(corpus, word2id_freq)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å®Œæˆè¯­æ–™æ•°æ®é¢„å¤„ç†ä¹‹åï¼Œéœ€è¦æ„é€ è®­ç»ƒæ•°æ®ã€‚æ ¹æ®ä¸Šé¢çš„æè¿°ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€ä¸ªæ»‘åŠ¨çª—å£å¯¹è¯­æ–™ä»å·¦åˆ°å³æ‰«æï¼Œåœ¨æ¯ä¸ªçª—å£å†…ï¼Œä¸­å¿ƒè¯éœ€è¦é¢„æµ‹å®ƒçš„ä¸Šä¸‹æ–‡ï¼Œå¹¶å½¢æˆè®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "åœ¨å®é™…æ“ä½œä¸­ï¼Œç”±äºè¯è¡¨å¾€å¾€å¾ˆå¤§ï¼ˆ50000ï¼Œ100000ç­‰ï¼‰ï¼Œå¯¹å¤§è¯è¡¨çš„ä¸€äº›çŸ©é˜µè¿ç®—ï¼ˆå¦‚softmaxï¼‰éœ€è¦æ¶ˆè€—å·¨å¤§çš„èµ„æºï¼Œå› æ­¤å¯ä»¥é€šè¿‡è´Ÿé‡‡æ ·çš„æ–¹å¼æ¨¡æ‹Ÿsoftmaxçš„ç»“æœï¼Œä»£ç å®ç°å¦‚ä¸‹ã€‚\n",
    "* ç»™å®šä¸€ä¸ªä¸­å¿ƒè¯å’Œä¸€ä¸ªéœ€è¦é¢„æµ‹çš„ä¸Šä¸‹æ–‡è¯ï¼ŒæŠŠè¿™ä¸ªä¸Šä¸‹æ–‡è¯ä½œä¸ºæ­£æ ·æœ¬ã€‚\n",
    "* é€šè¿‡è¯è¡¨éšæœºé‡‡æ ·çš„æ–¹å¼ï¼Œé€‰æ‹©è‹¥å¹²ä¸ªè´Ÿæ ·æœ¬ã€‚\n",
    "* æŠŠä¸€ä¸ªå¤§è§„æ¨¡åˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ª2åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡è¿™ç§æ–¹å¼ä¼˜åŒ–è®¡ç®—é€Ÿåº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.203478Z",
     "start_time": "2022-07-13T09:39:25.359415Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:02:19.989348Z",
     "iopub.status.busy": "2022-07-13T08:02:19.988717Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "500000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1200000\n",
      "1300000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2200000\n",
      "2300000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3900000\n",
      "4000000\n",
      "4500000\n",
      "4600000\n",
      "4800000\n",
      "5000000\n",
      "5100000\n",
      "5800000\n",
      "6000000\n",
      "6400000\n",
      "7000000\n",
      "7100000\n",
      "7500000\n",
      "7600000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8400000\n",
      "8500000\n",
      "center_word anarchism, target originated, label 1\n",
      "center_word anarchism, target cockney, label 0\n",
      "center_word anarchism, target rapacity, label 0\n",
      "center_word anarchism, target aubame, label 0\n",
      "center_word anarchism, target olgivanna, label 0\n",
      "center_word anarchism, target as, label 1\n",
      "center_word anarchism, target trkk, label 0\n",
      "center_word anarchism, target voicedvoiceless, label 0\n",
      "center_word anarchism, target aretz, label 0\n",
      "center_word anarchism, target hcrt, label 0\n",
      "center_word anarchism, target term, label 1\n",
      "center_word anarchism, target weissbiers, label 0\n",
      "center_word anarchism, target storyline, label 0\n",
      "center_word anarchism, target edb, label 0\n",
      "center_word anarchism, target hotbed, label 0\n",
      "center_word term, target originated, label 1\n",
      "center_word term, target ruga, label 0\n",
      "center_word term, target zuek, label 0\n",
      "center_word term, target jourgensen, label 0\n",
      "center_word term, target bazeries, label 0\n",
      "center_word term, target as, label 1\n",
      "center_word term, target bellare, label 0\n",
      "center_word term, target saffuri, label 0\n",
      "center_word term, target tkabir, label 0\n",
      "center_word term, target unfixes, label 0\n",
      "center_word term, target abuse, label 1\n",
      "center_word term, target cbarks, label 0\n",
      "center_word term, target dagashi, label 0\n",
      "center_word term, target waringhien, label 0\n",
      "center_word term, target headspins, label 0\n",
      "center_word term, target against, label 1\n",
      "center_word term, target danjaq, label 0\n",
      "center_word term, target shadows, label 0\n",
      "center_word term, target jeetendra, label 0\n",
      "center_word term, target sadyattes, label 0\n",
      "center_word against, target as, label 1\n",
      "center_word against, target retiarius, label 0\n",
      "center_word against, target dxf, label 0\n",
      "center_word against, target autofluorescence, label 0\n",
      "center_word against, target godstitans, label 0\n",
      "center_word against, target term, label 1\n",
      "center_word against, target nawn, label 0\n",
      "center_word against, target intersection, label 0\n",
      "center_word against, target tapadh, label 0\n",
      "center_word against, target belorado, label 0\n",
      "center_word against, target abuse, label 1\n",
      "center_word against, target benziman, label 0\n",
      "center_word against, target kintalion, label 0\n",
      "center_word against, target helmets, label 0\n",
      "center_word against, target jumbo, label 0\n"
     ]
    }
   ],
   "source": [
    "#æ„é€ æ•°æ®ï¼Œå‡†å¤‡æ¨¡å‹è®­ç»ƒ\n",
    "#max_window_sizeä»£è¡¨äº†æœ€å¤§çš„window_sizeçš„å¤§å°ï¼Œç¨‹åºä¼šæ ¹æ®max_window_sizeä»å·¦åˆ°å³æ‰«ææ•´ä¸ªè¯­æ–™\n",
    "#negative_sample_numä»£è¡¨äº†å¯¹äºæ¯ä¸ªæ­£æ ·æœ¬ï¼Œæˆ‘ä»¬éœ€è¦éšæœºé‡‡æ ·å¤šå°‘è´Ÿæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œ\n",
    "#ä¸€èˆ¬æ¥è¯´ï¼Œnegative_sample_numçš„å€¼è¶Šå¤§ï¼Œè®­ç»ƒæ•ˆæœè¶Šç¨³å®šï¼Œä½†æ˜¯è®­ç»ƒé€Ÿåº¦è¶Šæ…¢ã€‚ \n",
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, \n",
    "               negative_sample_num = 4):\n",
    "    \n",
    "    #ä½¿ç”¨ä¸€ä¸ªlistå­˜å‚¨å¤„ç†å¥½çš„æ•°æ®\n",
    "    dataset = []\n",
    "    center_word_idx=0\n",
    "\n",
    "    #ä»å·¦åˆ°å³ï¼Œå¼€å§‹æšä¸¾æ¯ä¸ªä¸­å¿ƒç‚¹çš„ä½ç½®\n",
    "    while center_word_idx < len(corpus):\n",
    "        #ä»¥max_window_sizeä¸ºä¸Šé™ï¼Œéšæœºé‡‡æ ·ä¸€ä¸ªwindow_sizeï¼Œè¿™æ ·ä¼šä½¿å¾—è®­ç»ƒæ›´åŠ ç¨³å®š\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        #å½“å‰çš„ä¸­å¿ƒè¯å°±æ˜¯center_word_idxæ‰€æŒ‡å‘çš„è¯ï¼Œå¯ä»¥å½“ä½œæ­£æ ·æœ¬\n",
    "        positive_word = corpus[center_word_idx]\n",
    "\n",
    "        #ä»¥å½“å‰ä¸­å¿ƒè¯ä¸ºä¸­å¿ƒï¼Œå·¦å³ä¸¤ä¾§åœ¨window_sizeå†…çš„è¯å°±æ˜¯ä¸Šä¸‹æ–‡\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\n",
    "\n",
    "        #å¯¹äºæ¯ä¸ªæ­£æ ·æœ¬æ¥è¯´ï¼Œéšæœºé‡‡æ ·negative_sample_numä¸ªè´Ÿæ ·æœ¬ï¼Œç”¨äºè®­ç»ƒ\n",
    "        for context_word in context_word_candidates:\n",
    "            #é¦–å…ˆæŠŠï¼ˆä¸Šä¸‹æ–‡ï¼Œæ­£æ ·æœ¬ï¼Œlabel=1ï¼‰çš„ä¸‰å…ƒç»„æ•°æ®æ”¾å…¥datasetä¸­ï¼Œ\n",
    "            #è¿™é‡Œlabel=1è¡¨ç¤ºè¿™ä¸ªæ ·æœ¬æ˜¯ä¸ªæ­£æ ·æœ¬\n",
    "            dataset.append((positive_word, context_word, 1))\n",
    "\n",
    "            #å¼€å§‹è´Ÿé‡‡æ ·\n",
    "            i = 0\n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate is not context_word:\n",
    "                    #æŠŠï¼ˆä¸Šä¸‹æ–‡ï¼Œè´Ÿæ ·æœ¬ï¼Œlabel=0ï¼‰çš„ä¸‰å…ƒç»„æ•°æ®æ”¾å…¥datasetä¸­ï¼Œ\n",
    "                    #è¿™é‡Œlabel=0è¡¨ç¤ºè¿™ä¸ªæ ·æœ¬æ˜¯ä¸ªè´Ÿæ ·æœ¬\n",
    "                    dataset.append((positive_word, negative_word_candidate, 0))\n",
    "                    i += 1\n",
    "        \n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\n",
    "        if center_word_idx == (len(corpus) - 1):\n",
    "            center_word_idx += 1\n",
    "        if center_word_idx % 100000 == 0:\n",
    "            print(center_word_idx)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = build_data(corpus, word2id_dict, word2id_freq)\n",
    "for _, (context_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[context_word],\n",
    "                                                   id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " è®­ç»ƒæ•°æ®å‡†å¤‡å¥½åï¼ŒæŠŠè®­ç»ƒæ•°æ®éƒ½ç»„è£…æˆmini-batchï¼Œå¹¶å‡†å¤‡è¾“å…¥åˆ°ç½‘ç»œä¸­è¿›è¡Œè®­ç»ƒï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.234305Z",
     "start_time": "2022-07-13T09:40:58.204760Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:12:02.502034Z",
     "iopub.status.busy": "2022-07-13T08:12:02.501354Z",
     "iopub.status.idle": "2022-07-13T08:12:02.510935Z",
     "shell.execute_reply": "2022-07-13T08:12:02.510037Z",
     "shell.execute_reply.started": "2022-07-13T08:12:02.501991Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#æ„é€ mini-batchï¼Œå‡†å¤‡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "#æˆ‘ä»¬å°†ä¸åŒç±»å‹çš„æ•°æ®æ”¾åˆ°ä¸åŒçš„tensoré‡Œï¼Œä¾¿äºç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†\n",
    "#å¹¶é€šè¿‡numpyçš„arrayå‡½æ•°ï¼Œæ„é€ å‡ºä¸åŒçš„tensoræ¥ï¼Œå¹¶æŠŠè¿™äº›tensoré€å…¥ç¥ç»ç½‘ç»œä¸­è¿›è¡Œè®­ç»ƒ\n",
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    \n",
    "    #center_word_batchç¼“å­˜batch_sizeä¸ªä¸­å¿ƒè¯\n",
    "    center_word_batch = []\n",
    "    #target_word_batchç¼“å­˜batch_sizeä¸ªç›®æ ‡è¯ï¼ˆå¯ä»¥æ˜¯æ­£æ ·æœ¬æˆ–è€…è´Ÿæ ·æœ¬ï¼‰\n",
    "    target_word_batch = []\n",
    "    #label_batchç¼“å­˜äº†batch_sizeä¸ª0æˆ–1çš„æ ‡ç­¾ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ\n",
    "    label_batch = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        #æ¯æ¬¡å¼€å¯ä¸€ä¸ªæ–°epochä¹‹å‰ï¼Œéƒ½å¯¹æ•°æ®è¿›è¡Œä¸€æ¬¡éšæœºæ‰“ä¹±ï¼Œæé«˜è®­ç»ƒæ•ˆæœ\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        for center_word, target_word, label in dataset:\n",
    "            #éå†datasetä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼Œå¹¶å°†è¿™äº›æ•°æ®é€åˆ°ä¸åŒçš„tensoré‡Œ\n",
    "            center_word_batch.append([center_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "\n",
    "            #å½“æ ·æœ¬ç§¯æ”’åˆ°ä¸€ä¸ªbatch_sizeåï¼Œæˆ‘ä»¬æŠŠæ•°æ®éƒ½è¿”å›å›æ¥\n",
    "            #åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨numpyçš„arrayå‡½æ•°æŠŠlistå°è£…æˆtensor\n",
    "            #å¹¶ä½¿ç”¨pythonçš„è¿­ä»£å™¨æœºåˆ¶ï¼Œå°†æ•°æ®yieldå‡ºæ¥\n",
    "            #ä½¿ç”¨è¿­ä»£å™¨çš„å¥½å¤„æ˜¯å¯ä»¥èŠ‚çœå†…å­˜\n",
    "            if len(center_word_batch) == batch_size:\n",
    "                yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(target_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(label_batch).astype(\"float32\")\n",
    "                center_word_batch = []\n",
    "                target_word_batch = []\n",
    "                label_batch = []\n",
    "\n",
    "    if len(center_word_batch) > 0:\n",
    "        yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "            np.array(target_word_batch).astype(\"int64\"), \\\n",
    "            np.array(label_batch).astype(\"float32\")\n",
    "\n",
    "# for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç½‘ç»œå®šä¹‰\n",
    "\n",
    "å®šä¹‰cbowçš„ç½‘ç»œç»“æ„ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚åœ¨é£æ¡¨åŠ¨æ€å›¾ä¸­ï¼Œå¯¹äºä»»æ„ç½‘ç»œï¼Œéƒ½éœ€è¦å®šä¹‰ä¸€ä¸ªç»§æ‰¿è‡ªpaddle.nn.Layerçš„ç±»æ¥æ­å»ºç½‘ç»œç»“æ„ã€å‚æ•°ç­‰æ•°æ®çš„å£°æ˜ã€‚åŒæ—¶éœ€è¦åœ¨forwardå‡½æ•°ä¸­å®šä¹‰ç½‘ç»œçš„è®¡ç®—é€»è¾‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä»…éœ€è¦å®šä¹‰ç½‘ç»œçš„å‰å‘è®¡ç®—é€»è¾‘ï¼Œé£æ¡¨ä¼šè‡ªåŠ¨å®Œæˆç¥ç»ç½‘ç»œçš„åå‘è®¡ç®—ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.294869Z",
     "start_time": "2022-07-13T09:40:58.235821Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:12:08.601042Z",
     "iopub.status.busy": "2022-07-13T08:12:08.599970Z",
     "iopub.status.idle": "2022-07-13T08:12:08.611576Z",
     "shell.execute_reply": "2022-07-13T08:12:08.610641Z",
     "shell.execute_reply.started": "2022-07-13T08:12:08.600998Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#å®šä¹‰cbowè®­ç»ƒç½‘ç»œç»“æ„\n",
    "#è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯paddlepaddleçš„2.0.0ç‰ˆæœ¬\n",
    "#ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨ä½¿ç”¨nnè®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä¸€ä¸ªç±»æ¥å®šä¹‰ç½‘ç»œç»“æ„ï¼Œè¿™ä¸ªç±»ç»§æ‰¿äº†paddle.nn.Layer\n",
    "class CBOW(paddle.nn.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\n",
    "        #vocab_sizeå®šä¹‰äº†è¿™ä¸ªCBOWè¿™ä¸ªæ¨¡å‹çš„è¯è¡¨å¤§å°\n",
    "        #embedding_sizeå®šä¹‰äº†è¯å‘é‡çš„ç»´åº¦æ˜¯å¤šå°‘\n",
    "        #init_scaleå®šä¹‰äº†è¯å‘é‡åˆå§‹åŒ–çš„èŒƒå›´ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæ¯”è¾ƒå°çš„åˆå§‹åŒ–èŒƒå›´æœ‰åŠ©äºæ¨¡å‹è®­ç»ƒ\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        #ä½¿ç”¨paddle.nnæä¾›çš„Embeddingå‡½æ•°ï¼Œæ„é€ ä¸€ä¸ªè¯å‘é‡å‚æ•°\n",
    "        #è¿™ä¸ªå‚æ•°çš„å¤§å°ä¸ºï¼šself.vocab_size, self.embedding_size\n",
    "        #è¿™ä¸ªå‚æ•°çš„åç§°ä¸ºï¼šembedding_para\n",
    "        #è¿™ä¸ªå‚æ•°çš„åˆå§‹åŒ–æ–¹å¼ä¸ºåœ¨[-init_scale, init_scale]åŒºé—´è¿›è¡Œå‡åŒ€é‡‡æ ·\n",
    "        self.embedding = paddle.nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "        #ä½¿ç”¨paddle.nnæä¾›çš„Embeddingå‡½æ•°ï¼Œæ„é€ å¦å¤–ä¸€ä¸ªè¯å‘é‡å‚æ•°\n",
    "        #è¿™ä¸ªå‚æ•°çš„å¤§å°ä¸ºï¼šself.vocab_size, self.embedding_size\n",
    "        #è¿™ä¸ªå‚æ•°çš„åç§°ä¸ºï¼šembedding_para_out\n",
    "        #è¿™ä¸ªå‚æ•°çš„åˆå§‹åŒ–æ–¹å¼ä¸ºåœ¨[-init_scale, init_scale]åŒºé—´è¿›è¡Œå‡åŒ€é‡‡æ ·\n",
    "        #è·Ÿä¸Šé¢ä¸åŒçš„æ˜¯ï¼Œè¿™ä¸ªå‚æ•°çš„åç§°è·Ÿä¸Šé¢ä¸åŒï¼Œå› æ­¤ï¼Œ\n",
    "        #embedding_para_outå’Œembedding_paraè™½ç„¶æœ‰ç›¸åŒçš„shapeï¼Œä½†æ˜¯æƒé‡ä¸å…±äº«\n",
    "        self.embedding_out = paddle.nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_out_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "    #å®šä¹‰ç½‘ç»œçš„å‰å‘è®¡ç®—é€»è¾‘\n",
    "    #center_wordsæ˜¯ä¸€ä¸ªtensorï¼ˆmini-batchï¼‰ï¼Œè¡¨ç¤ºä¸­å¿ƒè¯\n",
    "    #target_wordsæ˜¯ä¸€ä¸ªtensorï¼ˆmini-batchï¼‰ï¼Œè¡¨ç¤ºç›®æ ‡è¯\n",
    "    #labelæ˜¯ä¸€ä¸ªtensorï¼ˆmini-batchï¼‰ï¼Œè¡¨ç¤ºè¿™ä¸ªè¯æ˜¯æ­£æ ·æœ¬è¿˜æ˜¯è´Ÿæ ·æœ¬ï¼ˆç”¨0æˆ–1è¡¨ç¤ºï¼‰\n",
    "    #ç”¨äºåœ¨è®­ç»ƒä¸­è®¡ç®—è¿™ä¸ªtensorä¸­å¯¹åº”è¯çš„åŒä¹‰è¯ï¼Œç”¨äºè§‚å¯Ÿæ¨¡å‹çš„è®­ç»ƒæ•ˆæœ\n",
    "    def forward(self, center_words, target_words, label):\n",
    "        #é¦–å…ˆï¼Œé€šè¿‡embedding_paraï¼ˆself.embeddingï¼‰å‚æ•°ï¼Œå°†mini-batchä¸­çš„è¯è½¬æ¢ä¸ºè¯å‘é‡\n",
    "        #è¿™é‡Œcenter_wordså’Œeval_words_embæŸ¥è¯¢çš„æ˜¯ä¸€ä¸ªç›¸åŒçš„å‚æ•°\n",
    "        #è€Œtarget_words_embæŸ¥è¯¢çš„æ˜¯å¦ä¸€ä¸ªå‚æ•°\n",
    "        center_words_emb = self.embedding(center_words)\n",
    "        target_words_emb = self.embedding_out(target_words)\n",
    "\n",
    "        #center_words_emb = [batch_size, embedding_size]\n",
    "        #target_words_emb = [batch_size, embedding_size]\n",
    "        #æˆ‘ä»¬é€šè¿‡ç‚¹ä¹˜çš„æ–¹å¼è®¡ç®—ä¸­å¿ƒè¯åˆ°ç›®æ ‡è¯çš„è¾“å‡ºæ¦‚ç‡ï¼Œå¹¶é€šè¿‡sigmoidå‡½æ•°ä¼°è®¡è¿™ä¸ªè¯æ˜¯æ­£æ ·æœ¬è¿˜æ˜¯è´Ÿæ ·æœ¬çš„æ¦‚ç‡ã€‚\n",
    "        word_sim = paddle.multiply(center_words_emb, target_words_emb)\n",
    "        word_sim = paddle.sum(word_sim, axis = -1)\n",
    "        word_sim = paddle.reshape(word_sim, shape=[-1])\n",
    "        pred = paddle.nn.functional.sigmoid(word_sim)\n",
    "\n",
    "        #é€šè¿‡ä¼°è®¡çš„è¾“å‡ºæ¦‚ç‡å®šä¹‰æŸå¤±å‡½æ•°ï¼Œæ³¨æ„æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯binary_cross_entropyå‡½æ•°\n",
    "        #å°†sigmoidè®¡ç®—å’Œcross entropyåˆå¹¶æˆä¸€æ­¥è®¡ç®—å¯ä»¥æ›´å¥½çš„ä¼˜åŒ–ï¼Œæ‰€ä»¥è¾“å…¥çš„æ˜¯word_simï¼Œè€Œä¸æ˜¯pred\n",
    "        \n",
    "        loss = paddle.nn.functional.binary_cross_entropy(paddle.nn.functional.sigmoid(word_sim), label)\n",
    "        loss = paddle.mean(loss)\n",
    "\n",
    "        #è¿”å›å‰å‘è®¡ç®—çš„ç»“æœï¼Œé£æ¡¨ä¼šé€šè¿‡backwardå‡½æ•°è‡ªåŠ¨è®¡ç®—å‡ºåå‘ç»“æœã€‚\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç½‘ç»œè®­ç»ƒ\n",
    "\n",
    "å®Œæˆç½‘ç»œå®šä¹‰åï¼Œå°±å¯ä»¥å¯åŠ¨æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬å®šä¹‰æ¯éš”100æ­¥æ‰“å°ä¸€æ¬¡Lossï¼Œä»¥ç¡®ä¿å½“å‰çš„ç½‘ç»œæ˜¯æ­£å¸¸æ”¶æ•›çš„ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ¯éš”1000æ­¥è§‚å¯Ÿä¸€ä¸‹cbowè®¡ç®—å‡ºæ¥çš„åŒä¹‰è¯ï¼ˆä½¿ç”¨ embeddingçš„ä¹˜ç§¯ï¼‰ï¼Œå¯è§†åŒ–ç½‘ç»œè®­ç»ƒæ•ˆæœï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.310037Z",
     "start_time": "2022-07-13T09:40:58.295374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T12:02:44.528529Z",
     "start_time": "2022-07-13T09:40:58.310547Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:12:13.759588Z",
     "iopub.status.busy": "2022-07-13T08:12:13.758825Z",
     "iopub.status.idle": "2022-07-13T08:26:37.819683Z",
     "shell.execute_reply": "2022-07-13T08:26:37.817084Z",
     "shell.execute_reply.started": "2022-07-13T08:12:13.759544Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, loss 0.693\n",
      "step 200, loss 0.693\n",
      "step 300, loss 0.693\n",
      "step 400, loss 0.692\n",
      "step 500, loss 0.691\n",
      "step 600, loss 0.687\n",
      "step 700, loss 0.683\n",
      "step 800, loss 0.668\n",
      "step 900, loss 0.650\n",
      "step 1000, loss 0.638\n",
      "step 1100, loss 0.623\n",
      "step 1200, loss 0.590\n",
      "step 1300, loss 0.583\n",
      "step 1400, loss 0.548\n",
      "step 1500, loss 0.551\n",
      "step 1600, loss 0.539\n",
      "step 1700, loss 0.508\n",
      "step 1800, loss 0.475\n",
      "step 1900, loss 0.475\n",
      "step 2000, loss 0.438\n",
      "å•è¯1 king å’Œå•è¯2 queen çš„cosç»“æœä¸º 0.935353\n",
      "å•è¯1 she å’Œå•è¯2 her çš„cosç»“æœä¸º 0.955853\n",
      "å•è¯1 topic å’Œå•è¯2 theme çš„cosç»“æœä¸º 0.859415\n",
      "å•è¯1 woman å’Œå•è¯2 game çš„cosç»“æœä¸º 0.954040\n",
      "å•è¯1 one å’Œå•è¯2 name çš„cosç»“æœä¸º 0.955443\n",
      "step 2100, loss 0.409\n",
      "step 2200, loss 0.419\n",
      "step 2300, loss 0.409\n",
      "step 2400, loss 0.413\n",
      "step 2500, loss 0.380\n",
      "step 2600, loss 0.369\n",
      "step 2700, loss 0.383\n",
      "step 2800, loss 0.309\n",
      "step 2900, loss 0.366\n",
      "step 3000, loss 0.335\n",
      "step 3100, loss 0.353\n",
      "step 3200, loss 0.349\n",
      "step 3300, loss 0.311\n",
      "step 3400, loss 0.310\n",
      "step 3500, loss 0.317\n",
      "step 3600, loss 0.268\n",
      "step 3700, loss 0.320\n",
      "step 3800, loss 0.282\n",
      "step 3900, loss 0.274\n",
      "step 4000, loss 0.272\n",
      "å•è¯1 king å’Œå•è¯2 queen çš„cosç»“æœä¸º 0.939188\n",
      "å•è¯1 she å’Œå•è¯2 her çš„cosç»“æœä¸º 0.947604\n",
      "å•è¯1 topic å’Œå•è¯2 theme çš„cosç»“æœä¸º 0.922812\n",
      "å•è¯1 woman å’Œå•è¯2 game çš„cosç»“æœä¸º 0.963822\n",
      "å•è¯1 one å’Œå•è¯2 name çš„cosç»“æœä¸º 0.921967\n",
      "step 4100, loss 0.296\n",
      "step 4200, loss 0.297\n",
      "step 4300, loss 0.285\n",
      "step 4400, loss 0.264\n",
      "step 4500, loss 0.264\n",
      "step 4600, loss 0.254\n",
      "step 4700, loss 0.261\n",
      "step 4800, loss 0.255\n",
      "step 4900, loss 0.250\n",
      "step 5000, loss 0.262\n",
      "step 5100, loss 0.203\n",
      "step 5200, loss 0.217\n",
      "step 5300, loss 0.237\n",
      "step 5400, loss 0.287\n",
      "step 5500, loss 0.232\n",
      "step 5600, loss 0.267\n",
      "step 5700, loss 0.253\n",
      "step 5800, loss 0.278\n",
      "step 5900, loss 0.281\n",
      "step 6000, loss 0.233\n",
      "å•è¯1 king å’Œå•è¯2 queen çš„cosç»“æœä¸º 0.924169\n",
      "å•è¯1 she å’Œå•è¯2 her çš„cosç»“æœä¸º 0.930072\n",
      "å•è¯1 topic å’Œå•è¯2 theme çš„cosç»“æœä¸º 0.901349\n",
      "å•è¯1 woman å’Œå•è¯2 game çš„cosç»“æœä¸º 0.953883\n",
      "å•è¯1 one å’Œå•è¯2 name çš„cosç»“æœä¸º 0.859788\n",
      "step 6100, loss 0.257\n",
      "step 6200, loss 0.190\n",
      "step 6300, loss 0.230\n",
      "step 6400, loss 0.213\n",
      "step 6500, loss 0.226\n",
      "step 6600, loss 0.230\n",
      "step 6700, loss 0.264\n",
      "step 6800, loss 0.260\n",
      "step 6900, loss 0.221\n",
      "step 7000, loss 0.229\n",
      "step 7100, loss 0.201\n",
      "step 7200, loss 0.235\n",
      "step 7300, loss 0.213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f7e7d01e1cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#é€šè¿‡minimizeå‡½æ•°ï¼Œè®©ç¨‹åºæ ¹æ®lossï¼Œå®Œæˆä¸€æ­¥å¯¹å‚æ•°çš„ä¼˜åŒ–æ›´æ–°\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0madam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m#ä½¿ç”¨clear_gradientså‡½æ•°æ¸…ç©ºæ¨¡å‹ä¸­çš„æ¢¯åº¦ï¼Œä»¥ä¾¿äºä¸‹ä¸€ä¸ªmini-batchè¿›è¡Œæ›´æ–°\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mskip_gram_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-369>\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, startup_program, parameters, no_grad_set)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\fluid\\dygraph\\base.py\u001b[0m in \u001b[0;36m__impl__\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, startup_program, parameters, no_grad_set)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         optimize_ops = self._apply_optimize(\n\u001b[1;32m-> 1179\u001b[1;33m             loss, startup_program=startup_program, params_grads=params_grads)\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moptimize_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36m_apply_optimize\u001b[1;34m(self, loss, startup_program, params_grads)\u001b[0m\n\u001b[0;32m    961\u001b[0m                     params_grads['params'] = self.append_regularization_ops(\n\u001b[0;32m    962\u001b[0m                         params_grads['params'], self.regularization)\n\u001b[1;32m--> 963\u001b[1;33m                 \u001b[0moptimize_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_optimization_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[0mprogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[1;34m(self, parameters_and_grads)\u001b[0m\n\u001b[0;32m    765\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m                             self._append_optimize_op(target_block,\n\u001b[1;32m--> 767\u001b[1;33m                                                      param_and_grad)\n\u001b[0m\u001b[0;32m    768\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mparam_and_grad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\adam.py\u001b[0m in \u001b[0;36m_append_optimize_op\u001b[1;34m(self, block, param_and_grad)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[1;34m'epsilon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lazy_mode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;34m'min_row_size_to_use_multithread'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'beta1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_beta1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 'beta2', _beta2, 'multi_precision', find_master)\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#å¼€å§‹è®­ç»ƒï¼Œå®šä¹‰ä¸€äº›è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä½¿ç”¨çš„è¶…å‚æ•°\n",
    "batch_size = 512\n",
    "epoch_num = 3\n",
    "embedding_size = 200\n",
    "step = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "#å®šä¹‰ä¸€ä¸ªä½¿ç”¨word-embeddingè®¡ç®—cosçš„å‡½æ•°\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"å•è¯1 %s å’Œå•è¯2 %s çš„cosç»“æœä¸º %f\" %(query1_token, query2_token, cos))\n",
    "\n",
    "\n",
    "#é€šè¿‡æˆ‘ä»¬å®šä¹‰çš„CBOWç±»ï¼Œæ¥æ„é€ ä¸€ä¸ªcbowæ¨¡å‹ç½‘ç»œ\n",
    "skip_gram_model = CBOW(vocab_size, embedding_size)\n",
    "#æ„é€ è®­ç»ƒè¿™ä¸ªç½‘ç»œçš„ä¼˜åŒ–å™¨\n",
    "adam = paddle.optimizer.Adam(learning_rate=learning_rate, parameters = skip_gram_model.parameters())\n",
    "\n",
    "#ä½¿ç”¨build_batchå‡½æ•°ï¼Œä»¥mini-batchä¸ºå•ä½ï¼Œéå†è®­ç»ƒæ•°æ®ï¼Œå¹¶è®­ç»ƒç½‘ç»œ\n",
    "for center_words, target_words, label in build_batch(\n",
    "    dataset, batch_size, epoch_num):\n",
    "    #ä½¿ç”¨paddle.to_tensorå‡½æ•°ï¼Œå°†ä¸€ä¸ªnumpyçš„tensorï¼Œè½¬æ¢ä¸ºé£æ¡¨å¯è®¡ç®—çš„tensor\n",
    "    center_words_var = paddle.to_tensor(center_words)\n",
    "    target_words_var = paddle.to_tensor(target_words)\n",
    "    label_var = paddle.to_tensor(label)\n",
    "\n",
    "    #å°†è½¬æ¢åçš„tensoré€å…¥é£æ¡¨ä¸­ï¼Œè¿›è¡Œä¸€æ¬¡å‰å‘è®¡ç®—ï¼Œå¹¶å¾—åˆ°è®¡ç®—ç»“æœ\n",
    "    pred, loss = skip_gram_model(\n",
    "        center_words_var, target_words_var, label_var)\n",
    "\n",
    "    #é€šè¿‡backwardå‡½æ•°ï¼Œè®©ç¨‹åºè‡ªåŠ¨å®Œæˆåå‘è®¡ç®—\n",
    "    loss.backward()\n",
    "    #é€šè¿‡minimizeå‡½æ•°ï¼Œè®©ç¨‹åºæ ¹æ®lossï¼Œå®Œæˆä¸€æ­¥å¯¹å‚æ•°çš„ä¼˜åŒ–æ›´æ–°\n",
    "    adam.minimize(loss)\n",
    "    #ä½¿ç”¨clear_gradientså‡½æ•°æ¸…ç©ºæ¨¡å‹ä¸­çš„æ¢¯åº¦ï¼Œä»¥ä¾¿äºä¸‹ä¸€ä¸ªmini-batchè¿›è¡Œæ›´æ–°\n",
    "    skip_gram_model.clear_gradients()\n",
    "\n",
    "    #æ¯ç»è¿‡100ä¸ªmini-batchï¼Œæ‰“å°ä¸€æ¬¡å½“å‰çš„lossï¼Œçœ‹çœ‹lossæ˜¯å¦åœ¨ç¨³å®šä¸‹é™\n",
    "    step += 1\n",
    "    if step % 100 == 0:\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "    #ç»è¿‡10000ä¸ªmini-batchï¼Œæ‰“å°ä¸€æ¬¡æ¨¡å‹å¯¹eval_wordsä¸­çš„10ä¸ªè¯è®¡ç®—çš„åŒä¹‰è¯\n",
    "    #è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨è¯å’Œè¯ä¹‹é—´çš„å‘é‡ç‚¹ç§¯ä½œä¸ºè¡¡é‡ç›¸ä¼¼åº¦çš„æ–¹æ³•\n",
    "    #æˆ‘ä»¬åªæ‰“å°äº†5ä¸ªæœ€ç›¸ä¼¼çš„è¯\n",
    "    if step % 2000 == 0:\n",
    "        embedding_matrix = skip_gram_model.embedding.weight.numpy()\n",
    "        np.save(\"./embedding\", embedding_matrix)\n",
    "        get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "        get_cos(\"she\",\"her\",embedding_matrix)\n",
    "        get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "        get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "        get_cos(\"one\",\"name\",embedding_matrix)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»æ‰“å°ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡ä¸€å®šæ­¥éª¤çš„è®­ç»ƒï¼ŒLossé€æ¸ä¸‹é™å¹¶è¶‹äºç¨³å®šã€‚\n",
    "### ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—è¯„ä»·è¯å‘é‡ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T12:02:44.536966Z",
     "start_time": "2022-07-13T09:38:42.985Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#å®šä¹‰ä¸€ä¸ªä½¿ç”¨word-embeddingè®¡ç®—cosçš„å‡½æ•°\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"å•è¯1 %s å’Œå•è¯2 %s çš„cosç»“æœä¸º %f\" %(query1_token, query2_token, cos) )\n",
    "\n",
    "embedding_matrix = np.load('embedding.npy') \n",
    "get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "get_cos(\"she\",\"her\",embedding_matrix)\n",
    "get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "get_cos(\"one\",\"name\",embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
