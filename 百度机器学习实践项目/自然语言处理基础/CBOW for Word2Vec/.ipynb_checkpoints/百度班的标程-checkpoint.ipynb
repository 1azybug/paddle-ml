{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理任务中，词向量是表示自然语言里单词的一种方法，即把每个词都表示为一个N维空间内的点，即一个高维空间内的向量。通过这种方法，实现把自然语言计算转换为向量计算。\n",
    "\n",
    "如 **图1** 所示的词向量计算任务中，先把每个词（如queen，king等）转换成一个高维空间的向量，这些向量在一定意义上可以代表这个词的语义信息。再通过计算这些向量之间的距离，就可以计算出词语之间的关联关系，从而达到让计算机像计算数值一样去计算自然语言的目的。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/401409b5b36f4c55ade6078ba198634ee54b8f27e51d4729b66b32e844689969\" width=\"1000\" ></center>\n",
    "<center>图1：词向量计算示意图</center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec包含两个经典模型，CBOW（Continuous Bag-of-Words）和Skip-gram，如 **图2** 所示。\n",
    "\n",
    "- **CBOW**：通过上下文的词向量推理中心词。\n",
    "- **Skip-gram**：根据中心词推理上下文。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5d0fc1ad327e438e9101d059772be6732ee808bd80e04e1eabc575cd28e61a69\" width=\"1000\" ></center>\n",
    "<center><br>图2：CBOW和Skip-gram语义学习示意图</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW的算法实现\n",
    "\n",
    "我们以这句话：“Pineapples are spiked and yellow”为例介绍CBOW算法实现。\n",
    "\n",
    "如 **图3** 所示，CBOW是一个具有3层结构的神经网络，分别是：\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/72397490c0ba499692cff31484431c57bc9d20f7ef344454868e12d628ec5bd3\" width=\"400\" ></center>\n",
    "<center><br>图3：CBOW的算法实现</br></center>\n",
    "<br></br>\n",
    "\n",
    "* **输入层：** 一个形状为C×V的one-hot张量，其中C代表上线文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示，比如“Pineapples, are, and, yellow”。\n",
    "* **隐藏层：** 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。\n",
    "* **输出层：** 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率：\n",
    "\n",
    "$$𝑠𝑜𝑓𝑡𝑚𝑎𝑥({O_i})= \\frac{exp({O_i})}{\\sum_jexp({O_j})}$$\n",
    "\n",
    "在实际操作中，使用一个滑动窗口（一般情况下，长度是奇数），从左到右开始扫描当前句子。每个扫描出来的片段被当成一个小句子，每个小句子中间的词被认为是中心词，其余的词被认为是这个中心词的上下文。\n",
    "\n",
    "### CBOW的实际实现\n",
    "\n",
    "在实际中，为避免过于庞大的计算量，我们通常采用负采样的方法，来避免查询整个此表，从而将多分类问题转换为二分类问题。具体实现过程**如图6**：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/517260aee7f24b97bb8e42774daf1b4ee5aaa5824d004ed1854a04bde4a1c8ac)\n",
    "<center><br>图6：CBOW算法的实际实现</br></center>\n",
    "<br></br>\n",
    "\n",
    "在实现的过程中，通常会让模型接收3个tensor输入：\n",
    "\n",
    "- 代表上下文单词的tensor：假设我们称之为context_words $V$，一般来说，这个tensor是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中每个中心词具体的ID。\n",
    "\n",
    "- 代表目标词的tensor：假设我们称之为target_words $T$，一般来说，这个tensor同样是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中每个目标词具体的ID。\n",
    "\n",
    "- 代表目标词标签的tensor：假设我们称之为labels $L$，一般来说，这个tensor是一个形状为[batch_size, 1]的tensor，每个元素不是0就是1（0：负样本，1：正样本）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用飞桨实现 CBOW\n",
    "\n",
    "接下来我们将学习使用飞桨实现CBOW模型的方法。在飞桨中，不同深度学习模型的训练过程基本一致，流程如下：\n",
    "\n",
    "1. **数据处理**：选择需要使用的数据，并做好必要的预处理工作。\n",
    "\n",
    "2. **网络定义**：使用飞桨定义好网络结构，包括输入层，中间层，输出层，损失函数和优化算法。\n",
    "\n",
    "3. **网络训练**：将准备好的数据送入神经网络进行学习，并观察学习的过程是否正常，如损失函数值是否在降低，也可以打印一些中间步骤的结果出来等。\n",
    "\n",
    "4. **网络评估**：使用测试集合测试训练好的神经网络，看看训练效果如何。\n",
    "\n",
    "在数据处理前，需要先加载飞桨平台（如果用户在本地使用，请确保已经安装飞桨）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:38:45.087221Z",
     "start_time": "2022-07-13T09:38:42.970576Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:22.023816Z",
     "iopub.status.busy": "2022-07-13T08:01:22.023142Z",
     "iopub.status.idle": "2022-07-13T08:01:23.271266Z",
     "shell.execute_reply": "2022-07-13T08:01:23.270261Z",
     "shell.execute_reply.started": "2022-07-13T08:01:22.023782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "\n",
    "首先，找到一个合适的语料用于训练word2vec模型。我们选择text8数据集，这个数据集里包含了大量从维基百科收集到的英文语料，我们可以通过如下代码下载数据集，下载后的文件被保存在当前目录的text8.txt文件内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:08.428933Z",
     "start_time": "2022-07-13T09:38:45.088222Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:25.708232Z",
     "iopub.status.busy": "2022-07-13T08:01:25.707231Z",
     "iopub.status.idle": "2022-07-13T08:01:30.548619Z",
     "shell.execute_reply": "2022-07-13T08:01:30.547755Z",
     "shell.execute_reply.started": "2022-07-13T08:01:25.708195Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#下载语料用来训练word2vec\n",
    "def download():\n",
    "    #可以从百度云服务器下载一些开源数据集（dataset.bj.bcebos.com）\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\n",
    "    #使用python的requests包下载数据集到本地\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "    #把下载后的文件存储在当前目录的text8.txt文件内\n",
    "    with open(\"./text8.txt\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，把下载的语料读取到程序里，并打印前500个字符看看语料的样子，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:08.938083Z",
     "start_time": "2022-07-13T09:39:08.429441Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:35.085243Z",
     "iopub.status.busy": "2022-07-13T08:01:35.084263Z",
     "iopub.status.idle": "2022-07-13T08:01:35.269646Z",
     "shell.execute_reply": "2022-07-13T08:01:35.268851Z",
     "shell.execute_reply.started": "2022-07-13T08:01:35.085209Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "#读取text8数据\n",
    "def load_text8():\n",
    "    with open(\"./text8.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = load_text8()\n",
    "\n",
    "#打印前500个字符，简要看一下这个语料的样子\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，在自然语言处理中，需要先对语料进行切词。对于英文来说，可以比较简单地直接使用空格进行切词，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:09.700647Z",
     "start_time": "2022-07-13T09:39:08.938590Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:38.777836Z",
     "iopub.status.busy": "2022-07-13T08:01:38.777232Z",
     "iopub.status.idle": "2022-07-13T08:01:40.199516Z",
     "shell.execute_reply": "2022-07-13T08:01:40.198706Z",
     "shell.execute_reply.started": "2022-07-13T08:01:38.777789Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "#对语料进行预处理（分词）\n",
    "def data_preprocess(corpus):\n",
    "    #由于英文单词出现在句首的时候经常要大写，所以我们把所有英文字符都转换为小写，\n",
    "    #以便对语料进行归一化处理（Apple vs apple等）\n",
    "    corpus = corpus.strip().lower()\n",
    "    corpus = corpus.split(\" \")\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = data_preprocess(corpus)\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在经过切词后，需要对语料进行统计，为每个词构造ID。一般来说，可以根据每个词在语料中出现的频次构造ID，频次越高，ID越小，便于对词典进行管理。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:12.903938Z",
     "start_time": "2022-07-13T09:39:09.701155Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:45.348350Z",
     "iopub.status.busy": "2022-07-13T08:01:45.347672Z",
     "iopub.status.idle": "2022-07-13T08:01:50.341158Z",
     "shell.execute_reply": "2022-07-13T08:01:50.340194Z",
     "shell.execute_reply.started": "2022-07-13T08:01:45.348312Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 253854 different words in the corpus\n",
      "word the, its id 0, its word freq 1061396\n",
      "word of, its id 1, its word freq 593677\n",
      "word and, its id 2, its word freq 416629\n",
      "word one, its id 3, its word freq 411764\n",
      "word in, its id 4, its word freq 372201\n",
      "word a, its id 5, its word freq 325873\n",
      "word to, its id 6, its word freq 316376\n",
      "word zero, its id 7, its word freq 264975\n",
      "word nine, its id 8, its word freq 250430\n",
      "word two, its id 9, its word freq 192644\n",
      "word is, its id 10, its word freq 183153\n",
      "word as, its id 11, its word freq 131815\n",
      "word eight, its id 12, its word freq 125285\n",
      "word for, its id 13, its word freq 118445\n",
      "word s, its id 14, its word freq 116710\n",
      "word five, its id 15, its word freq 115789\n",
      "word three, its id 16, its word freq 114775\n",
      "word was, its id 17, its word freq 112807\n",
      "word by, its id 18, its word freq 111831\n",
      "word that, its id 19, its word freq 109510\n",
      "word four, its id 20, its word freq 108182\n",
      "word six, its id 21, its word freq 102145\n",
      "word seven, its id 22, its word freq 99683\n",
      "word with, its id 23, its word freq 95603\n",
      "word on, its id 24, its word freq 91250\n",
      "word are, its id 25, its word freq 76527\n",
      "word it, its id 26, its word freq 73334\n",
      "word from, its id 27, its word freq 72871\n",
      "word or, its id 28, its word freq 68945\n",
      "word his, its id 29, its word freq 62603\n",
      "word an, its id 30, its word freq 61925\n",
      "word be, its id 31, its word freq 61281\n",
      "word this, its id 32, its word freq 58832\n",
      "word which, its id 33, its word freq 54788\n",
      "word at, its id 34, its word freq 54576\n",
      "word he, its id 35, its word freq 53573\n",
      "word also, its id 36, its word freq 44358\n",
      "word not, its id 37, its word freq 44033\n",
      "word have, its id 38, its word freq 39712\n",
      "word were, its id 39, its word freq 39086\n",
      "word has, its id 40, its word freq 37866\n",
      "word but, its id 41, its word freq 35358\n",
      "word other, its id 42, its word freq 32433\n",
      "word their, its id 43, its word freq 31523\n",
      "word its, its id 44, its word freq 29567\n",
      "word first, its id 45, its word freq 28810\n",
      "word they, its id 46, its word freq 28553\n",
      "word some, its id 47, its word freq 28161\n",
      "word had, its id 48, its word freq 28100\n",
      "word all, its id 49, its word freq 26229\n"
     ]
    }
   ],
   "source": [
    "#构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus):\n",
    "    #首先统计每个不同词的频率（出现的次数），使用一个词典记录\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    #将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\n",
    "    #一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    #构造3个不同的词典，分别存储，\n",
    "    #每个词到id的映射关系：word2id_dict\n",
    "    #每个id出现的频率：word2id_freq\n",
    "    #每个id到词典映射关系：id2word_dict\n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "    id2word_dict = dict()\n",
    "\n",
    "    #按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict)\n",
    "        word2id_dict[word] = curr_id\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "        id2word_dict[curr_id] = word\n",
    "\n",
    "    return word2id_freq, word2id_dict, id2word_dict\n",
    "\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到word2id词典后，我们还需要进一步处理原始语料，把每个词替换成对应的ID，便于神经网络进行处理，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:14.328516Z",
     "start_time": "2022-07-13T09:39:12.905462Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:01:55.467687Z",
     "iopub.status.busy": "2022-07-13T08:01:55.467100Z",
     "iopub.status.idle": "2022-07-13T08:01:57.975794Z",
     "shell.execute_reply": "2022-07-13T08:01:57.975005Z",
     "shell.execute_reply.started": "2022-07-13T08:01:55.467649Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 tokens in the corpus\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580, 0, 194, 10, 190, 58, 4, 5, 10712, 214, 6, 1324, 104, 454, 19, 58, 2731, 362, 6, 3672, 0]\n"
     ]
    }
   ],
   "source": [
    "#把语料转换为id序列\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    #使用一个循环，将语料中的每个词替换成对应的id，以便于神经网络进行处理\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，需要使用二次采样法处理原始文本。二次采样法的主要思想是降低高频词在语料中出现的频次，降低的方法是随机将高频的词抛弃，频率越高，被抛弃的概率就越高，频率越低，被抛弃的概率就越低，这样像标点符号或冠词这样的高频词就会被抛弃，从而优化整个词表的词向量训练效果，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:39:25.358903Z",
     "start_time": "2022-07-13T09:39:14.329518Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:02:03.856561Z",
     "iopub.status.busy": "2022-07-13T08:02:03.855655Z",
     "iopub.status.idle": "2022-07-13T08:02:14.220977Z",
     "shell.execute_reply": "2022-07-13T08:02:14.220151Z",
     "shell.execute_reply.started": "2022-07-13T08:02:03.856528Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8743238 tokens in the corpus\n",
      "[5233, 3080, 11, 194, 3133, 155, 741, 10571, 27349, 854, 15067, 58112, 854, 3580, 194, 10712, 214, 1324, 454, 58, 2731, 362, 3672, 708, 539, 1423, 2757, 567, 686, 7088, 5233, 1052, 0, 320, 248, 44611, 2877, 792, 5233, 602, 1134, 2621, 8983, 279, 4147, 141, 25, 6437, 4186, 5233]\n"
     ]
    }
   ],
   "source": [
    "#使用二次采样算法（subsampling）处理语料，强化训练效果\n",
    "def subsampling(corpus, word2id_freq):\n",
    "    \n",
    "    #这个discard函数决定了一个词会不会被替换，这个函数是具有随机性的，每次调用结果不同\n",
    "    #如果一个词的频率很大，那么它被遗弃的概率就很大\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\n",
    "\n",
    "    corpus = [word for word in corpus if not discard(word)]\n",
    "    return corpus\n",
    "\n",
    "corpus = subsampling(corpus, word2id_freq)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成语料数据预处理之后，需要构造训练数据。根据上面的描述，我们需要使用一个滑动窗口对语料从左到右扫描，在每个窗口内，中心词需要预测它的上下文，并形成训练数据。\n",
    "\n",
    "在实际操作中，由于词表往往很大（50000，100000等），对大词表的一些矩阵运算（如softmax）需要消耗巨大的资源，因此可以通过负采样的方式模拟softmax的结果，代码实现如下。\n",
    "* 给定一个中心词和一个需要预测的上下文词，把这个上下文词作为正样本。\n",
    "* 通过词表随机采样的方式，选择若干个负样本。\n",
    "* 把一个大规模分类问题转化为一个2分类问题，通过这种方式优化计算速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.203478Z",
     "start_time": "2022-07-13T09:39:25.359415Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:02:19.989348Z",
     "iopub.status.busy": "2022-07-13T08:02:19.988717Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "500000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1200000\n",
      "1300000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2200000\n",
      "2300000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3900000\n",
      "4000000\n",
      "4500000\n",
      "4600000\n",
      "4800000\n",
      "5000000\n",
      "5100000\n",
      "5800000\n",
      "6000000\n",
      "6400000\n",
      "7000000\n",
      "7100000\n",
      "7500000\n",
      "7600000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8400000\n",
      "8500000\n",
      "center_word anarchism, target originated, label 1\n",
      "center_word anarchism, target cockney, label 0\n",
      "center_word anarchism, target rapacity, label 0\n",
      "center_word anarchism, target aubame, label 0\n",
      "center_word anarchism, target olgivanna, label 0\n",
      "center_word anarchism, target as, label 1\n",
      "center_word anarchism, target trkk, label 0\n",
      "center_word anarchism, target voicedvoiceless, label 0\n",
      "center_word anarchism, target aretz, label 0\n",
      "center_word anarchism, target hcrt, label 0\n",
      "center_word anarchism, target term, label 1\n",
      "center_word anarchism, target weissbiers, label 0\n",
      "center_word anarchism, target storyline, label 0\n",
      "center_word anarchism, target edb, label 0\n",
      "center_word anarchism, target hotbed, label 0\n",
      "center_word term, target originated, label 1\n",
      "center_word term, target ruga, label 0\n",
      "center_word term, target zuek, label 0\n",
      "center_word term, target jourgensen, label 0\n",
      "center_word term, target bazeries, label 0\n",
      "center_word term, target as, label 1\n",
      "center_word term, target bellare, label 0\n",
      "center_word term, target saffuri, label 0\n",
      "center_word term, target tkabir, label 0\n",
      "center_word term, target unfixes, label 0\n",
      "center_word term, target abuse, label 1\n",
      "center_word term, target cbarks, label 0\n",
      "center_word term, target dagashi, label 0\n",
      "center_word term, target waringhien, label 0\n",
      "center_word term, target headspins, label 0\n",
      "center_word term, target against, label 1\n",
      "center_word term, target danjaq, label 0\n",
      "center_word term, target shadows, label 0\n",
      "center_word term, target jeetendra, label 0\n",
      "center_word term, target sadyattes, label 0\n",
      "center_word against, target as, label 1\n",
      "center_word against, target retiarius, label 0\n",
      "center_word against, target dxf, label 0\n",
      "center_word against, target autofluorescence, label 0\n",
      "center_word against, target godstitans, label 0\n",
      "center_word against, target term, label 1\n",
      "center_word against, target nawn, label 0\n",
      "center_word against, target intersection, label 0\n",
      "center_word against, target tapadh, label 0\n",
      "center_word against, target belorado, label 0\n",
      "center_word against, target abuse, label 1\n",
      "center_word against, target benziman, label 0\n",
      "center_word against, target kintalion, label 0\n",
      "center_word against, target helmets, label 0\n",
      "center_word against, target jumbo, label 0\n"
     ]
    }
   ],
   "source": [
    "#构造数据，准备模型训练\n",
    "#max_window_size代表了最大的window_size的大小，程序会根据max_window_size从左到右扫描整个语料\n",
    "#negative_sample_num代表了对于每个正样本，我们需要随机采样多少负样本用于训练，\n",
    "#一般来说，negative_sample_num的值越大，训练效果越稳定，但是训练速度越慢。 \n",
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, \n",
    "               negative_sample_num = 4):\n",
    "    \n",
    "    #使用一个list存储处理好的数据\n",
    "    dataset = []\n",
    "    center_word_idx=0\n",
    "\n",
    "    #从左到右，开始枚举每个中心点的位置\n",
    "    while center_word_idx < len(corpus):\n",
    "        #以max_window_size为上限，随机采样一个window_size，这样会使得训练更加稳定\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        #当前的中心词就是center_word_idx所指向的词，可以当作正样本\n",
    "        positive_word = corpus[center_word_idx]\n",
    "\n",
    "        #以当前中心词为中心，左右两侧在window_size内的词就是上下文\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\n",
    "\n",
    "        #对于每个正样本来说，随机采样negative_sample_num个负样本，用于训练\n",
    "        for context_word in context_word_candidates:\n",
    "            #首先把（上下文，正样本，label=1）的三元组数据放入dataset中，\n",
    "            #这里label=1表示这个样本是个正样本\n",
    "            dataset.append((positive_word, context_word, 1))\n",
    "\n",
    "            #开始负采样\n",
    "            i = 0\n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate is not context_word:\n",
    "                    #把（上下文，负样本，label=0）的三元组数据放入dataset中，\n",
    "                    #这里label=0表示这个样本是个负样本\n",
    "                    dataset.append((positive_word, negative_word_candidate, 0))\n",
    "                    i += 1\n",
    "        \n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\n",
    "        if center_word_idx == (len(corpus) - 1):\n",
    "            center_word_idx += 1\n",
    "        if center_word_idx % 100000 == 0:\n",
    "            print(center_word_idx)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = build_data(corpus, word2id_dict, word2id_freq)\n",
    "for _, (context_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[context_word],\n",
    "                                                   id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 训练数据准备好后，把训练数据都组装成mini-batch，并准备输入到网络中进行训练，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.234305Z",
     "start_time": "2022-07-13T09:40:58.204760Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:12:02.502034Z",
     "iopub.status.busy": "2022-07-13T08:12:02.501354Z",
     "iopub.status.idle": "2022-07-13T08:12:02.510935Z",
     "shell.execute_reply": "2022-07-13T08:12:02.510037Z",
     "shell.execute_reply.started": "2022-07-13T08:12:02.501991Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#构造mini-batch，准备对模型进行训练\n",
    "#我们将不同类型的数据放到不同的tensor里，便于神经网络进行处理\n",
    "#并通过numpy的array函数，构造出不同的tensor来，并把这些tensor送入神经网络中进行训练\n",
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    \n",
    "    #center_word_batch缓存batch_size个中心词\n",
    "    center_word_batch = []\n",
    "    #target_word_batch缓存batch_size个目标词（可以是正样本或者负样本）\n",
    "    target_word_batch = []\n",
    "    #label_batch缓存了batch_size个0或1的标签，用于模型训练\n",
    "    label_batch = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        #每次开启一个新epoch之前，都对数据进行一次随机打乱，提高训练效果\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        for center_word, target_word, label in dataset:\n",
    "            #遍历dataset中的每个样本，并将这些数据送到不同的tensor里\n",
    "            center_word_batch.append([center_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "\n",
    "            #当样本积攒到一个batch_size后，我们把数据都返回回来\n",
    "            #在这里我们使用numpy的array函数把list封装成tensor\n",
    "            #并使用python的迭代器机制，将数据yield出来\n",
    "            #使用迭代器的好处是可以节省内存\n",
    "            if len(center_word_batch) == batch_size:\n",
    "                yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(target_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(label_batch).astype(\"float32\")\n",
    "                center_word_batch = []\n",
    "                target_word_batch = []\n",
    "                label_batch = []\n",
    "\n",
    "    if len(center_word_batch) > 0:\n",
    "        yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "            np.array(target_word_batch).astype(\"int64\"), \\\n",
    "            np.array(label_batch).astype(\"float32\")\n",
    "\n",
    "# for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络定义\n",
    "\n",
    "定义cbow的网络结构，用于模型训练。在飞桨动态图中，对于任意网络，都需要定义一个继承自paddle.nn.Layer的类来搭建网络结构、参数等数据的声明。同时需要在forward函数中定义网络的计算逻辑。值得注意的是，我们仅需要定义网络的前向计算逻辑，飞桨会自动完成神经网络的反向计算，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.294869Z",
     "start_time": "2022-07-13T09:40:58.235821Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:12:08.601042Z",
     "iopub.status.busy": "2022-07-13T08:12:08.599970Z",
     "iopub.status.idle": "2022-07-13T08:12:08.611576Z",
     "shell.execute_reply": "2022-07-13T08:12:08.610641Z",
     "shell.execute_reply.started": "2022-07-13T08:12:08.600998Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#定义cbow训练网络结构\n",
    "#这里我们使用的是paddlepaddle的2.0.0版本\n",
    "#一般来说，在使用nn训练的时候，我们需要通过一个类来定义网络结构，这个类继承了paddle.nn.Layer\n",
    "class CBOW(paddle.nn.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\n",
    "        #vocab_size定义了这个CBOW这个模型的词表大小\n",
    "        #embedding_size定义了词向量的维度是多少\n",
    "        #init_scale定义了词向量初始化的范围，一般来说，比较小的初始化范围有助于模型训练\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        #使用paddle.nn提供的Embedding函数，构造一个词向量参数\n",
    "        #这个参数的大小为：self.vocab_size, self.embedding_size\n",
    "        #这个参数的名称为：embedding_para\n",
    "        #这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\n",
    "        self.embedding = paddle.nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "        #使用paddle.nn提供的Embedding函数，构造另外一个词向量参数\n",
    "        #这个参数的大小为：self.vocab_size, self.embedding_size\n",
    "        #这个参数的名称为：embedding_para_out\n",
    "        #这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\n",
    "        #跟上面不同的是，这个参数的名称跟上面不同，因此，\n",
    "        #embedding_para_out和embedding_para虽然有相同的shape，但是权重不共享\n",
    "        self.embedding_out = paddle.nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_out_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "    #定义网络的前向计算逻辑\n",
    "    #center_words是一个tensor（mini-batch），表示中心词\n",
    "    #target_words是一个tensor（mini-batch），表示目标词\n",
    "    #label是一个tensor（mini-batch），表示这个词是正样本还是负样本（用0或1表示）\n",
    "    #用于在训练中计算这个tensor中对应词的同义词，用于观察模型的训练效果\n",
    "    def forward(self, center_words, target_words, label):\n",
    "        #首先，通过embedding_para（self.embedding）参数，将mini-batch中的词转换为词向量\n",
    "        #这里center_words和eval_words_emb查询的是一个相同的参数\n",
    "        #而target_words_emb查询的是另一个参数\n",
    "        center_words_emb = self.embedding(center_words)\n",
    "        target_words_emb = self.embedding_out(target_words)\n",
    "\n",
    "        #center_words_emb = [batch_size, embedding_size]\n",
    "        #target_words_emb = [batch_size, embedding_size]\n",
    "        #我们通过点乘的方式计算中心词到目标词的输出概率，并通过sigmoid函数估计这个词是正样本还是负样本的概率。\n",
    "        word_sim = paddle.multiply(center_words_emb, target_words_emb)\n",
    "        word_sim = paddle.sum(word_sim, axis = -1)\n",
    "        word_sim = paddle.reshape(word_sim, shape=[-1])\n",
    "        pred = paddle.nn.functional.sigmoid(word_sim)\n",
    "\n",
    "        #通过估计的输出概率定义损失函数，注意我们使用的是binary_cross_entropy函数\n",
    "        #将sigmoid计算和cross entropy合并成一步计算可以更好的优化，所以输入的是word_sim，而不是pred\n",
    "        \n",
    "        loss = paddle.nn.functional.binary_cross_entropy(paddle.nn.functional.sigmoid(word_sim), label)\n",
    "        loss = paddle.mean(loss)\n",
    "\n",
    "        #返回前向计算的结果，飞桨会通过backward函数自动计算出反向结果。\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络训练\n",
    "\n",
    "完成网络定义后，就可以启动模型训练。我们定义每隔100步打印一次Loss，以确保当前的网络是正常收敛的。同时，我们每隔1000步观察一下cbow计算出来的同义词（使用 embedding的乘积），可视化网络训练效果，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T09:40:58.310037Z",
     "start_time": "2022-07-13T09:40:58.295374Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T12:02:44.528529Z",
     "start_time": "2022-07-13T09:40:58.310547Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:12:13.759588Z",
     "iopub.status.busy": "2022-07-13T08:12:13.758825Z",
     "iopub.status.idle": "2022-07-13T08:26:37.819683Z",
     "shell.execute_reply": "2022-07-13T08:26:37.817084Z",
     "shell.execute_reply.started": "2022-07-13T08:12:13.759544Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, loss 0.693\n",
      "step 200, loss 0.693\n",
      "step 300, loss 0.693\n",
      "step 400, loss 0.692\n",
      "step 500, loss 0.691\n",
      "step 600, loss 0.687\n",
      "step 700, loss 0.683\n",
      "step 800, loss 0.668\n",
      "step 900, loss 0.650\n",
      "step 1000, loss 0.638\n",
      "step 1100, loss 0.623\n",
      "step 1200, loss 0.590\n",
      "step 1300, loss 0.583\n",
      "step 1400, loss 0.548\n",
      "step 1500, loss 0.551\n",
      "step 1600, loss 0.539\n",
      "step 1700, loss 0.508\n",
      "step 1800, loss 0.475\n",
      "step 1900, loss 0.475\n",
      "step 2000, loss 0.438\n",
      "单词1 king 和单词2 queen 的cos结果为 0.935353\n",
      "单词1 she 和单词2 her 的cos结果为 0.955853\n",
      "单词1 topic 和单词2 theme 的cos结果为 0.859415\n",
      "单词1 woman 和单词2 game 的cos结果为 0.954040\n",
      "单词1 one 和单词2 name 的cos结果为 0.955443\n",
      "step 2100, loss 0.409\n",
      "step 2200, loss 0.419\n",
      "step 2300, loss 0.409\n",
      "step 2400, loss 0.413\n",
      "step 2500, loss 0.380\n",
      "step 2600, loss 0.369\n",
      "step 2700, loss 0.383\n",
      "step 2800, loss 0.309\n",
      "step 2900, loss 0.366\n",
      "step 3000, loss 0.335\n",
      "step 3100, loss 0.353\n",
      "step 3200, loss 0.349\n",
      "step 3300, loss 0.311\n",
      "step 3400, loss 0.310\n",
      "step 3500, loss 0.317\n",
      "step 3600, loss 0.268\n",
      "step 3700, loss 0.320\n",
      "step 3800, loss 0.282\n",
      "step 3900, loss 0.274\n",
      "step 4000, loss 0.272\n",
      "单词1 king 和单词2 queen 的cos结果为 0.939188\n",
      "单词1 she 和单词2 her 的cos结果为 0.947604\n",
      "单词1 topic 和单词2 theme 的cos结果为 0.922812\n",
      "单词1 woman 和单词2 game 的cos结果为 0.963822\n",
      "单词1 one 和单词2 name 的cos结果为 0.921967\n",
      "step 4100, loss 0.296\n",
      "step 4200, loss 0.297\n",
      "step 4300, loss 0.285\n",
      "step 4400, loss 0.264\n",
      "step 4500, loss 0.264\n",
      "step 4600, loss 0.254\n",
      "step 4700, loss 0.261\n",
      "step 4800, loss 0.255\n",
      "step 4900, loss 0.250\n",
      "step 5000, loss 0.262\n",
      "step 5100, loss 0.203\n",
      "step 5200, loss 0.217\n",
      "step 5300, loss 0.237\n",
      "step 5400, loss 0.287\n",
      "step 5500, loss 0.232\n",
      "step 5600, loss 0.267\n",
      "step 5700, loss 0.253\n",
      "step 5800, loss 0.278\n",
      "step 5900, loss 0.281\n",
      "step 6000, loss 0.233\n",
      "单词1 king 和单词2 queen 的cos结果为 0.924169\n",
      "单词1 she 和单词2 her 的cos结果为 0.930072\n",
      "单词1 topic 和单词2 theme 的cos结果为 0.901349\n",
      "单词1 woman 和单词2 game 的cos结果为 0.953883\n",
      "单词1 one 和单词2 name 的cos结果为 0.859788\n",
      "step 6100, loss 0.257\n",
      "step 6200, loss 0.190\n",
      "step 6300, loss 0.230\n",
      "step 6400, loss 0.213\n",
      "step 6500, loss 0.226\n",
      "step 6600, loss 0.230\n",
      "step 6700, loss 0.264\n",
      "step 6800, loss 0.260\n",
      "step 6900, loss 0.221\n",
      "step 7000, loss 0.229\n",
      "step 7100, loss 0.201\n",
      "step 7200, loss 0.235\n",
      "step 7300, loss 0.213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f7e7d01e1cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#通过minimize函数，让程序根据loss，完成一步对参数的优化更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0madam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m#使用clear_gradients函数清空模型中的梯度，以便于下一个mini-batch进行更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mskip_gram_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-369>\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, startup_program, parameters, no_grad_set)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\fluid\\dygraph\\base.py\u001b[0m in \u001b[0;36m__impl__\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, startup_program, parameters, no_grad_set)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         optimize_ops = self._apply_optimize(\n\u001b[1;32m-> 1179\u001b[1;33m             loss, startup_program=startup_program, params_grads=params_grads)\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moptimize_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36m_apply_optimize\u001b[1;34m(self, loss, startup_program, params_grads)\u001b[0m\n\u001b[0;32m    961\u001b[0m                     params_grads['params'] = self.append_regularization_ops(\n\u001b[0;32m    962\u001b[0m                         params_grads['params'], self.regularization)\n\u001b[1;32m--> 963\u001b[1;33m                 \u001b[0moptimize_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_optimization_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[0mprogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[1;34m(self, parameters_and_grads)\u001b[0m\n\u001b[0;32m    765\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m                             self._append_optimize_op(target_block,\n\u001b[1;32m--> 767\u001b[1;33m                                                      param_and_grad)\n\u001b[0m\u001b[0;32m    768\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mparam_and_grad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\paddle\\optimizer\\adam.py\u001b[0m in \u001b[0;36m_append_optimize_op\u001b[1;34m(self, block, param_and_grad)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[1;34m'epsilon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lazy_mode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;34m'min_row_size_to_use_multithread'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'beta1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_beta1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 'beta2', _beta2, 'multi_precision', find_master)\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#开始训练，定义一些训练过程中需要使用的超参数\n",
    "batch_size = 512\n",
    "epoch_num = 3\n",
    "embedding_size = 200\n",
    "step = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "#定义一个使用word-embedding计算cos的函数\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"单词1 %s 和单词2 %s 的cos结果为 %f\" %(query1_token, query2_token, cos))\n",
    "\n",
    "\n",
    "#通过我们定义的CBOW类，来构造一个cbow模型网络\n",
    "skip_gram_model = CBOW(vocab_size, embedding_size)\n",
    "#构造训练这个网络的优化器\n",
    "adam = paddle.optimizer.Adam(learning_rate=learning_rate, parameters = skip_gram_model.parameters())\n",
    "\n",
    "#使用build_batch函数，以mini-batch为单位，遍历训练数据，并训练网络\n",
    "for center_words, target_words, label in build_batch(\n",
    "    dataset, batch_size, epoch_num):\n",
    "    #使用paddle.to_tensor函数，将一个numpy的tensor，转换为飞桨可计算的tensor\n",
    "    center_words_var = paddle.to_tensor(center_words)\n",
    "    target_words_var = paddle.to_tensor(target_words)\n",
    "    label_var = paddle.to_tensor(label)\n",
    "\n",
    "    #将转换后的tensor送入飞桨中，进行一次前向计算，并得到计算结果\n",
    "    pred, loss = skip_gram_model(\n",
    "        center_words_var, target_words_var, label_var)\n",
    "\n",
    "    #通过backward函数，让程序自动完成反向计算\n",
    "    loss.backward()\n",
    "    #通过minimize函数，让程序根据loss，完成一步对参数的优化更新\n",
    "    adam.minimize(loss)\n",
    "    #使用clear_gradients函数清空模型中的梯度，以便于下一个mini-batch进行更新\n",
    "    skip_gram_model.clear_gradients()\n",
    "\n",
    "    #每经过100个mini-batch，打印一次当前的loss，看看loss是否在稳定下降\n",
    "    step += 1\n",
    "    if step % 100 == 0:\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "    #经过10000个mini-batch，打印一次模型对eval_words中的10个词计算的同义词\n",
    "    #这里我们使用词和词之间的向量点积作为衡量相似度的方法\n",
    "    #我们只打印了5个最相似的词\n",
    "    if step % 2000 == 0:\n",
    "        embedding_matrix = skip_gram_model.embedding.weight.numpy()\n",
    "        np.save(\"./embedding\", embedding_matrix)\n",
    "        get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "        get_cos(\"she\",\"her\",embedding_matrix)\n",
    "        get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "        get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "        get_cos(\"one\",\"name\",embedding_matrix)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从打印结果可以看到，经过一定步骤的训练，Loss逐渐下降并趋于稳定。\n",
    "### 余弦相似度计算评价词向量结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T12:02:44.536966Z",
     "start_time": "2022-07-13T09:38:42.985Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#定义一个使用word-embedding计算cos的函数\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"单词1 %s 和单词2 %s 的cos结果为 %f\" %(query1_token, query2_token, cos) )\n",
    "\n",
    "embedding_matrix = np.load('embedding.npy') \n",
    "get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "get_cos(\"she\",\"her\",embedding_matrix)\n",
    "get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "get_cos(\"one\",\"name\",embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
