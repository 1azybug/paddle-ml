{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:51.092061Z",
     "iopub.status.busy": "2022-07-13T12:00:51.091744Z",
     "iopub.status.idle": "2022-07-13T12:00:52.882422Z",
     "shell.execute_reply": "2022-07-13T12:00:52.881458Z",
     "shell.execute_reply.started": "2022-07-13T12:00:51.092033Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import io \n",
    "import sys \n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math \n",
    "import numpy as np \n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:52.884417Z",
     "iopub.status.busy": "2022-07-13T12:00:52.883830Z",
     "iopub.status.idle": "2022-07-13T12:00:52.890405Z",
     "shell.execute_reply": "2022-07-13T12:00:52.889503Z",
     "shell.execute_reply.started": "2022-07-13T12:00:52.884383Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download():\n",
    "    corpus_url='https://dataset.bj.bcebos.com/word2vec/text8.txt'\n",
    "    web_requests=requests.get(corpus_url)\n",
    "    corpus=web_requests.content\n",
    "    with open('./text8.txt','wb') as f:\n",
    "        f.write(corpus)\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:52.892348Z",
     "iopub.status.busy": "2022-07-13T12:00:52.891928Z",
     "iopub.status.idle": "2022-07-13T12:00:58.395542Z",
     "shell.execute_reply": "2022-07-13T12:00:58.394237Z",
     "shell.execute_reply.started": "2022-07-13T12:00:52.892307Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:58.397547Z",
     "iopub.status.busy": "2022-07-13T12:00:58.397044Z",
     "iopub.status.idle": "2022-07-13T12:00:58.402032Z",
     "shell.execute_reply": "2022-07-13T12:00:58.401241Z",
     "shell.execute_reply.started": "2022-07-13T12:00:58.397517Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_text8():\n",
    "    with open('./text8.txt') as f:\n",
    "        corpus=f.read().strip('\\n')\n",
    "        f.close()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:58.403584Z",
     "iopub.status.busy": "2022-07-13T12:00:58.403181Z",
     "iopub.status.idle": "2022-07-13T12:00:58.772584Z",
     "shell.execute_reply": "2022-07-13T12:00:58.771328Z",
     "shell.execute_reply.started": "2022-07-13T12:00:58.403560Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus=load_text8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:58.775406Z",
     "iopub.status.busy": "2022-07-13T12:00:58.774552Z",
     "iopub.status.idle": "2022-07-13T12:00:58.789609Z",
     "shell.execute_reply": "2022-07-13T12:00:58.788318Z",
     "shell.execute_reply.started": "2022-07-13T12:00:58.775345Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:58.792392Z",
     "iopub.status.busy": "2022-07-13T12:00:58.791789Z",
     "iopub.status.idle": "2022-07-13T12:00:58.797687Z",
     "shell.execute_reply": "2022-07-13T12:00:58.796584Z",
     "shell.execute_reply.started": "2022-07-13T12:00:58.792347Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_preprocess(corpus):\n",
    "    corpus=corpus.strip().lower()\n",
    "    corpus=corpus.split(' ')\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:00:58.800089Z",
     "iopub.status.busy": "2022-07-13T12:00:58.799348Z",
     "iopub.status.idle": "2022-07-13T12:01:00.508691Z",
     "shell.execute_reply": "2022-07-13T12:01:00.507448Z",
     "shell.execute_reply.started": "2022-07-13T12:00:58.800047Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus=data_preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:00.510888Z",
     "iopub.status.busy": "2022-07-13T12:01:00.510405Z",
     "iopub.status.idle": "2022-07-13T12:01:00.516823Z",
     "shell.execute_reply": "2022-07-13T12:01:00.516027Z",
     "shell.execute_reply.started": "2022-07-13T12:01:00.510859Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sans',\n",
       " 'culottes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'french',\n",
       " 'revolution',\n",
       " 'whilst',\n",
       " 'the',\n",
       " 'term',\n",
       " 'is',\n",
       " 'still',\n",
       " 'used',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pejorative',\n",
       " 'way',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'any',\n",
       " 'act',\n",
       " 'that',\n",
       " 'used',\n",
       " 'violent',\n",
       " 'means',\n",
       " 'to',\n",
       " 'destroy',\n",
       " 'the']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:01.030349Z",
     "iopub.status.busy": "2022-07-13T12:01:01.029683Z",
     "iopub.status.idle": "2022-07-13T12:01:01.038034Z",
     "shell.execute_reply": "2022-07-13T12:01:01.037015Z",
     "shell.execute_reply.started": "2022-07-13T12:01:01.030306Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dict(corpus):\n",
    "    word_freq_dict={}\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word]=0\n",
    "        word_freq_dict[word]+=1\n",
    "    \n",
    "    word_freq_dict=sorted(word_freq_dict.items(),key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    word2id={}\n",
    "    id2freq={}\n",
    "    id2word={}\n",
    "    \n",
    "    for word,freq in word_freq_dict:\n",
    "        ind=len(word2id)\n",
    "        word2id[word]=ind\n",
    "        id2freq[ind]=freq\n",
    "        id2word[ind]=word\n",
    "    \n",
    "    return id2freq,word2id,id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:01.039560Z",
     "iopub.status.busy": "2022-07-13T12:01:01.039300Z",
     "iopub.status.idle": "2022-07-13T12:01:06.809600Z",
     "shell.execute_reply": "2022-07-13T12:01:06.808465Z",
     "shell.execute_reply.started": "2022-07-13T12:01:01.039538Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2freq,word2id,id2word=build_dict(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:06.811664Z",
     "iopub.status.busy": "2022-07-13T12:01:06.811165Z",
     "iopub.status.idle": "2022-07-13T12:01:06.815846Z",
     "shell.execute_reply": "2022-07-13T12:01:06.814907Z",
     "shell.execute_reply.started": "2022-07-13T12:01:06.811633Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size=len(id2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:06.817861Z",
     "iopub.status.busy": "2022-07-13T12:01:06.817150Z",
     "iopub.status.idle": "2022-07-13T12:01:06.822660Z",
     "shell.execute_reply": "2022-07-13T12:01:06.821997Z",
     "shell.execute_reply.started": "2022-07-13T12:01:06.817826Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:06.824436Z",
     "iopub.status.busy": "2022-07-13T12:01:06.823763Z",
     "iopub.status.idle": "2022-07-13T12:01:06.830647Z",
     "shell.execute_reply": "2022-07-13T12:01:06.829815Z",
     "shell.execute_reply.started": "2022-07-13T12:01:06.824408Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:the,its id:0,its freq:1061396\n",
      "word:of,its id:1,its freq:593677\n",
      "word:and,its id:2,its freq:416629\n",
      "word:one,its id:3,its freq:411764\n",
      "word:in,its id:4,its freq:372201\n",
      "word:a,its id:5,its freq:325873\n",
      "word:to,its id:6,its freq:316376\n",
      "word:zero,its id:7,its freq:264975\n"
     ]
    }
   ],
   "source": [
    "for _,(word,word_id) in zip(range(8),word2id.items()):\n",
    "    print(f'word:{word},its id:{word_id},its freq:{id2freq[word_id]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转化为id序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:06.832142Z",
     "iopub.status.busy": "2022-07-13T12:01:06.831793Z",
     "iopub.status.idle": "2022-07-13T12:01:09.890708Z",
     "shell.execute_reply": "2022-07-13T12:01:09.889758Z",
     "shell.execute_reply.started": "2022-07-13T12:01:06.832117Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus=[word2id[word] for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:09.892601Z",
     "iopub.status.busy": "2022-07-13T12:01:09.891995Z",
     "iopub.status.idle": "2022-07-13T12:01:09.897275Z",
     "shell.execute_reply": "2022-07-13T12:01:09.896602Z",
     "shell.execute_reply.started": "2022-07-13T12:01:09.892571Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5233, 3080, 11, 5, 194, 1, 3133, 45]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二次采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:09.898503Z",
     "iopub.status.busy": "2022-07-13T12:01:09.898200Z",
     "iopub.status.idle": "2022-07-13T12:01:09.903101Z",
     "shell.execute_reply": "2022-07-13T12:01:09.902432Z",
     "shell.execute_reply.started": "2022-07-13T12:01:09.898481Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def subsampling(corpus,id2freq):\n",
    "    \n",
    "    def discard(word_id):\n",
    "        q=1-math.sqrt(1e-4/id2freq[word_id]*len(corpus))\n",
    "        return random.uniform(0,1)<q\n",
    "    corpus=[word for word in corpus if not discard(word)]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:09.904253Z",
     "iopub.status.busy": "2022-07-13T12:01:09.903946Z",
     "iopub.status.idle": "2022-07-13T12:01:20.717497Z",
     "shell.execute_reply": "2022-07-13T12:01:20.716127Z",
     "shell.execute_reply.started": "2022-07-13T12:01:09.904231Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8744571\n",
      "[5233, 3080, 11, 194, 3133, 741, 476, 10571]\n"
     ]
    }
   ],
   "source": [
    "corpus=subsampling(corpus,id2freq)\n",
    "print(len(corpus))\n",
    "print(corpus[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:20.719791Z",
     "iopub.status.busy": "2022-07-13T12:01:20.719072Z",
     "iopub.status.idle": "2022-07-13T12:01:20.862049Z",
     "shell.execute_reply": "2022-07-13T12:01:20.860346Z",
     "shell.execute_reply.started": "2022-07-13T12:01:20.719759Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data(corpus,word2id,id2freq,max_window_size=3,negative_sample_num=4):\n",
    "    dataset=[]\n",
    "    center_word_idx=0\n",
    "    \n",
    "    while center_word_idx<len(corpus):\n",
    "        window_size=random.randint(1,max_window_size)\n",
    "        positive_word=corpus[center_word_idx]\n",
    "        \n",
    "        context_word_range=(max(0,center_word_idx-window_size),min(len(corpus)-1,center_word_idx+window_size))\n",
    "        context_word_candidates=[corpus[idx] for idx in range(context_word_range[0],context_word_range[1]+1) if idx!=center_word_idx]\n",
    "        \n",
    "        for context_word in context_word_candidates:\n",
    "            dataset.append((context_word,positive_word,1))\n",
    "            i=0\n",
    "            while i<negative_sample_num:\n",
    "                negative_word_candidate=random.randint(0,vocab_size-1)\n",
    "                if negative_word_candidate is not positive_word:\n",
    "                    dataset.append((context_word,negative_word_candidate,0))\n",
    "                    i+=1\n",
    "        \n",
    "        center_word_idx=min(len(corpus)-1,center_word_idx+window_size)\n",
    "        if center_word_idx==(len(corpus)-1):\n",
    "            center_word_idx+=1\n",
    "        if(center_word_idx%100000==0):\n",
    "            print(center_word_idx)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:01:20.864475Z",
     "iopub.status.busy": "2022-07-13T12:01:20.863913Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1600000\n",
      "2000000\n",
      "2200000\n",
      "2700000\n",
      "2800000\n",
      "3000000\n",
      "3100000\n",
      "3300000\n",
      "3900000\n",
      "4000000\n",
      "4200000\n",
      "4400000\n",
      "4600000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6300000\n",
      "6600000\n"
     ]
    }
   ],
   "source": [
    "dataset=build_data(corpus,word2id,id2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _,data in zip(range(11),dataset):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_batch(dataset,batch_size,epoch_num):\n",
    "    context_word_batch=[]\n",
    "    target_word_batch=[]\n",
    "    label_batch=[]\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        random.shuffle(dataset)\n",
    "        for context_word,target_word,label in dataset:\n",
    "            context_word_batch.append([context_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "            \n",
    "            if len(context_word_batch)==batch_size:\n",
    "                yield np.array(context_word_batch).astype('int64'),\\\n",
    "                    np.array(target_word_batch).astype('int64'),\\\n",
    "                    np.array(label_batch).astype('float32')\n",
    "                context_word_batch=[]\n",
    "                target_word_batch=[]\n",
    "                label_batch=[]\n",
    "        \n",
    "    if len(context_word_batch)>0:\n",
    "        yield np.array(context_word_batch).astype('int64'),\\\n",
    "            np.array(target_word_batch).astype('int64'),\\\n",
    "            np.array(label_batch).astype('float32')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CBOW(paddle.nn.Layer):\n",
    "    def __init__(self,vocab_size,embedding_size,init_scale=0.1):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.embedding=paddle.nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size,high=0.5/embedding_size\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.embedding_out=paddle.nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_out_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size,high=0.5/embedding_size\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self,context_words,target_words,label):\n",
    "        context_words_emb=self.embedding(context_words)\n",
    "        target_words_emb=self.embedding_out(target_words)\n",
    "        word_sim=paddle.multiply(context_words_emb,target_words_emb)\n",
    "        word_sim=paddle.sum(word_sim,axis=-1)\n",
    "        word_sim=paddle.reshape(word_sim,shape=[-1])\n",
    "        pred=paddle.nn.functional.sigmoid(word_sim)\n",
    "        loss=paddle.nn.functional.binary_cross_entropy(pred,label)\n",
    "        # print(f\"see loss:\\n{loss}\")\n",
    "        loss=paddle.mean(loss)\n",
    "        # print(f\"see loss after mean:\\n{loss}\")\n",
    "        return pred,loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "epoch_num=3\n",
    "embedding_size=200\n",
    "step=0\n",
    "learning_rate=0.001\n",
    "\n",
    "def get_cos(query1_token,query2_token,embed):\n",
    "    W=embed\n",
    "    x=W[word2id[query1_token]]\n",
    "    y=W[word2id[query2_token]]\n",
    "    cos=np.dot(x,y)/np.sqrt(np.sum(y*y)*np.sum(x*x)+1e-9)\n",
    "    flat=cos.flatten()\n",
    "    print(f\"{query1_token}和{query2_token}的cos结果为{cos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_gram_model=CBOW(vocab_size,embedding_size)\n",
    "adam=paddle.optimizer.Adam(learning_rate=learning_rate,parameters=skip_gram_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for context_words,target_words,label in build_batch(dataset,batch_size,epoch_num):\n",
    "    context_words_var=paddle.to_tensor(context_words)\n",
    "    target_words_var=paddle.to_tensor(target_words)\n",
    "    label_var=paddle.to_tensor(label)\n",
    "    \n",
    "    # print(f\"see target_words:\\n{target_words}\")\n",
    "    \n",
    "    pred,loss=skip_gram_model(context_words_var,target_words_var,label_var)\n",
    "    \n",
    "    loss.backward()\n",
    "    adam.minimize(loss)\n",
    "    skip_gram_model.clear_gradients()\n",
    "    \n",
    "    step+=1\n",
    "    if step%100==0:\n",
    "        print(f'step {step},loss {loss.numpy()[0]}')\n",
    "    \n",
    "    if step%2000==0:\n",
    "        embedding_matrix=skip_gram_model.embedding.weigth.numpy()\n",
    "        np.save('./embedding',embedding_matrix)\n",
    "        get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "        get_cos('she','her',embedding_matrix)\n",
    "        get_cos('topic','theme',embedding_matrix)\n",
    "        get_cos('woman','game',embedding_matrix)\n",
    "        get_cos('one','name',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
